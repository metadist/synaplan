<?php
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: model_config.proto

namespace Inference\ModelSequenceBatching;

use Google\Protobuf\Internal\GPBType;
use Google\Protobuf\Internal\RepeatedField;
use Google\Protobuf\Internal\GPBUtil;

/**
 *&#64;&#64;  .. cpp:var:: message StrategyOldest
 *&#64;&#64;
 *&#64;&#64;     The sequence batcher maintains up to 'max_candidate_sequences'
 *&#64;&#64;     candidate sequences. 'max_candidate_sequences' can be greater
 *&#64;&#64;     than the model's 'max_batch_size'. For inferencing the batcher
 *&#64;&#64;     chooses from the candidate sequences up to 'max_batch_size'
 *&#64;&#64;     inference requests. Requests are chosen in an oldest-first
 *&#64;&#64;     manner across all candidate sequences. A given sequence is
 *&#64;&#64;     not guaranteed to be assigned to the same batch slot for
 *&#64;&#64;     all inference requests of that sequence.
 *&#64;&#64;
 *
 * Generated from protobuf message <code>inference.ModelSequenceBatching.StrategyOldest</code>
 */
class StrategyOldest extends \Google\Protobuf\Internal\Message
{
    /**
     *&#64;&#64;    .. cpp:var:: int32 max_candidate_sequences
     *&#64;&#64;
     *&#64;&#64;       Maximum number of candidate sequences that the batcher
     *&#64;&#64;       maintains. Excess sequences are kept in an ordered backlog
     *&#64;&#64;       and become candidates when existing candidate sequences
     *&#64;&#64;       complete.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>int32 max_candidate_sequences = 1;</code>
     */
    protected $max_candidate_sequences = 0;
    /**
     *&#64;&#64;    .. cpp:var:: int32 preferred_batch_size (repeated)
     *&#64;&#64;
     *&#64;&#64;       Preferred batch sizes for dynamic batching of candidate
     *&#64;&#64;       sequences. If a batch of one of these sizes can be formed
     *&#64;&#64;       it will be executed immediately. If not specified a
     *&#64;&#64;       preferred batch size will be chosen automatically
     *&#64;&#64;       based on model and GPU characteristics.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>repeated int32 preferred_batch_size = 2;</code>
     */
    private $preferred_batch_size;
    /**
     *&#64;&#64;    .. cpp:var:: uint64 max_queue_delay_microseconds
     *&#64;&#64;
     *&#64;&#64;       The maximum time, in microseconds, a candidate request
     *&#64;&#64;       will be delayed in the dynamic batch scheduling queue to
     *&#64;&#64;       wait for additional requests for batching. Default is 0.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>uint64 max_queue_delay_microseconds = 3;</code>
     */
    protected $max_queue_delay_microseconds = 0;
    /**
     *&#64;&#64;    .. cpp:var:: bool preserve_ordering
     *&#64;&#64;
     *&#64;&#64;       Should the dynamic batcher preserve the ordering of responses to
     *&#64;&#64;       match the order of requests received by the scheduler. Default is
     *&#64;&#64;       false. If true, the responses will be returned in the same order
     *&#64;&#64;       as the order of requests sent to the scheduler. If false, the
     *&#64;&#64;       responses may be returned in arbitrary order. This option is
     *&#64;&#64;       specifically needed when a sequence of related inference requests
     *&#64;&#64;       (i.e. inference requests with the same correlation ID) are sent
     *&#64;&#64;       to the dynamic batcher to ensure that the sequence responses are
     *&#64;&#64;       in the correct order.
     *&#64;&#64;
     *&#64;&#64;       When using decoupled models, setting this to true may block the
     *&#64;&#64;       responses from independent sequences from being returned to the
     *&#64;&#64;       client until the previous request completes, hurting overall
     *&#64;&#64;       performance. If using GRPC streaming protocol, the stream
     *&#64;&#64;       ordering guarantee may be sufficient alone to ensure the
     *&#64;&#64;       responses for each sequence are returned in sequence-order
     *&#64;&#64;       without blocking based on independent requests, depending on the
     *&#64;&#64;       use case.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>bool preserve_ordering = 4;</code>
     */
    protected $preserve_ordering = false;

    /**
     * Constructor.
     *
     * @param array $data {
     *     Optional. Data for populating the Message object.
     *
     *     @type int $max_candidate_sequences
     *          &#64;&#64;    .. cpp:var:: int32 max_candidate_sequences
     *          &#64;&#64;
     *          &#64;&#64;       Maximum number of candidate sequences that the batcher
     *          &#64;&#64;       maintains. Excess sequences are kept in an ordered backlog
     *          &#64;&#64;       and become candidates when existing candidate sequences
     *          &#64;&#64;       complete.
     *          &#64;&#64;
     *     @type int[]|\Google\Protobuf\Internal\RepeatedField $preferred_batch_size
     *          &#64;&#64;    .. cpp:var:: int32 preferred_batch_size (repeated)
     *          &#64;&#64;
     *          &#64;&#64;       Preferred batch sizes for dynamic batching of candidate
     *          &#64;&#64;       sequences. If a batch of one of these sizes can be formed
     *          &#64;&#64;       it will be executed immediately. If not specified a
     *          &#64;&#64;       preferred batch size will be chosen automatically
     *          &#64;&#64;       based on model and GPU characteristics.
     *          &#64;&#64;
     *     @type int|string $max_queue_delay_microseconds
     *          &#64;&#64;    .. cpp:var:: uint64 max_queue_delay_microseconds
     *          &#64;&#64;
     *          &#64;&#64;       The maximum time, in microseconds, a candidate request
     *          &#64;&#64;       will be delayed in the dynamic batch scheduling queue to
     *          &#64;&#64;       wait for additional requests for batching. Default is 0.
     *          &#64;&#64;
     *     @type bool $preserve_ordering
     *          &#64;&#64;    .. cpp:var:: bool preserve_ordering
     *          &#64;&#64;
     *          &#64;&#64;       Should the dynamic batcher preserve the ordering of responses to
     *          &#64;&#64;       match the order of requests received by the scheduler. Default is
     *          &#64;&#64;       false. If true, the responses will be returned in the same order
     *          &#64;&#64;       as the order of requests sent to the scheduler. If false, the
     *          &#64;&#64;       responses may be returned in arbitrary order. This option is
     *          &#64;&#64;       specifically needed when a sequence of related inference requests
     *          &#64;&#64;       (i.e. inference requests with the same correlation ID) are sent
     *          &#64;&#64;       to the dynamic batcher to ensure that the sequence responses are
     *          &#64;&#64;       in the correct order.
     *          &#64;&#64;
     *          &#64;&#64;       When using decoupled models, setting this to true may block the
     *          &#64;&#64;       responses from independent sequences from being returned to the
     *          &#64;&#64;       client until the previous request completes, hurting overall
     *          &#64;&#64;       performance. If using GRPC streaming protocol, the stream
     *          &#64;&#64;       ordering guarantee may be sufficient alone to ensure the
     *          &#64;&#64;       responses for each sequence are returned in sequence-order
     *          &#64;&#64;       without blocking based on independent requests, depending on the
     *          &#64;&#64;       use case.
     *          &#64;&#64;
     * }
     */
    public function __construct($data = NULL) {
        \GPBMetadata\ModelConfig::initOnce();
        parent::__construct($data);
    }

    /**
     *&#64;&#64;    .. cpp:var:: int32 max_candidate_sequences
     *&#64;&#64;
     *&#64;&#64;       Maximum number of candidate sequences that the batcher
     *&#64;&#64;       maintains. Excess sequences are kept in an ordered backlog
     *&#64;&#64;       and become candidates when existing candidate sequences
     *&#64;&#64;       complete.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>int32 max_candidate_sequences = 1;</code>
     * @return int
     */
    public function getMaxCandidateSequences()
    {
        return $this->max_candidate_sequences;
    }

    /**
     *&#64;&#64;    .. cpp:var:: int32 max_candidate_sequences
     *&#64;&#64;
     *&#64;&#64;       Maximum number of candidate sequences that the batcher
     *&#64;&#64;       maintains. Excess sequences are kept in an ordered backlog
     *&#64;&#64;       and become candidates when existing candidate sequences
     *&#64;&#64;       complete.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>int32 max_candidate_sequences = 1;</code>
     * @param int $var
     * @return $this
     */
    public function setMaxCandidateSequences($var)
    {
        GPBUtil::checkInt32($var);
        $this->max_candidate_sequences = $var;

        return $this;
    }

    /**
     *&#64;&#64;    .. cpp:var:: int32 preferred_batch_size (repeated)
     *&#64;&#64;
     *&#64;&#64;       Preferred batch sizes for dynamic batching of candidate
     *&#64;&#64;       sequences. If a batch of one of these sizes can be formed
     *&#64;&#64;       it will be executed immediately. If not specified a
     *&#64;&#64;       preferred batch size will be chosen automatically
     *&#64;&#64;       based on model and GPU characteristics.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>repeated int32 preferred_batch_size = 2;</code>
     * @return \Google\Protobuf\Internal\RepeatedField
     */
    public function getPreferredBatchSize()
    {
        return $this->preferred_batch_size;
    }

    /**
     *&#64;&#64;    .. cpp:var:: int32 preferred_batch_size (repeated)
     *&#64;&#64;
     *&#64;&#64;       Preferred batch sizes for dynamic batching of candidate
     *&#64;&#64;       sequences. If a batch of one of these sizes can be formed
     *&#64;&#64;       it will be executed immediately. If not specified a
     *&#64;&#64;       preferred batch size will be chosen automatically
     *&#64;&#64;       based on model and GPU characteristics.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>repeated int32 preferred_batch_size = 2;</code>
     * @param int[]|\Google\Protobuf\Internal\RepeatedField $var
     * @return $this
     */
    public function setPreferredBatchSize($var)
    {
        $arr = GPBUtil::checkRepeatedField($var, \Google\Protobuf\Internal\GPBType::INT32);
        $this->preferred_batch_size = $arr;

        return $this;
    }

    /**
     *&#64;&#64;    .. cpp:var:: uint64 max_queue_delay_microseconds
     *&#64;&#64;
     *&#64;&#64;       The maximum time, in microseconds, a candidate request
     *&#64;&#64;       will be delayed in the dynamic batch scheduling queue to
     *&#64;&#64;       wait for additional requests for batching. Default is 0.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>uint64 max_queue_delay_microseconds = 3;</code>
     * @return int|string
     */
    public function getMaxQueueDelayMicroseconds()
    {
        return $this->max_queue_delay_microseconds;
    }

    /**
     *&#64;&#64;    .. cpp:var:: uint64 max_queue_delay_microseconds
     *&#64;&#64;
     *&#64;&#64;       The maximum time, in microseconds, a candidate request
     *&#64;&#64;       will be delayed in the dynamic batch scheduling queue to
     *&#64;&#64;       wait for additional requests for batching. Default is 0.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>uint64 max_queue_delay_microseconds = 3;</code>
     * @param int|string $var
     * @return $this
     */
    public function setMaxQueueDelayMicroseconds($var)
    {
        GPBUtil::checkUint64($var);
        $this->max_queue_delay_microseconds = $var;

        return $this;
    }

    /**
     *&#64;&#64;    .. cpp:var:: bool preserve_ordering
     *&#64;&#64;
     *&#64;&#64;       Should the dynamic batcher preserve the ordering of responses to
     *&#64;&#64;       match the order of requests received by the scheduler. Default is
     *&#64;&#64;       false. If true, the responses will be returned in the same order
     *&#64;&#64;       as the order of requests sent to the scheduler. If false, the
     *&#64;&#64;       responses may be returned in arbitrary order. This option is
     *&#64;&#64;       specifically needed when a sequence of related inference requests
     *&#64;&#64;       (i.e. inference requests with the same correlation ID) are sent
     *&#64;&#64;       to the dynamic batcher to ensure that the sequence responses are
     *&#64;&#64;       in the correct order.
     *&#64;&#64;
     *&#64;&#64;       When using decoupled models, setting this to true may block the
     *&#64;&#64;       responses from independent sequences from being returned to the
     *&#64;&#64;       client until the previous request completes, hurting overall
     *&#64;&#64;       performance. If using GRPC streaming protocol, the stream
     *&#64;&#64;       ordering guarantee may be sufficient alone to ensure the
     *&#64;&#64;       responses for each sequence are returned in sequence-order
     *&#64;&#64;       without blocking based on independent requests, depending on the
     *&#64;&#64;       use case.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>bool preserve_ordering = 4;</code>
     * @return bool
     */
    public function getPreserveOrdering()
    {
        return $this->preserve_ordering;
    }

    /**
     *&#64;&#64;    .. cpp:var:: bool preserve_ordering
     *&#64;&#64;
     *&#64;&#64;       Should the dynamic batcher preserve the ordering of responses to
     *&#64;&#64;       match the order of requests received by the scheduler. Default is
     *&#64;&#64;       false. If true, the responses will be returned in the same order
     *&#64;&#64;       as the order of requests sent to the scheduler. If false, the
     *&#64;&#64;       responses may be returned in arbitrary order. This option is
     *&#64;&#64;       specifically needed when a sequence of related inference requests
     *&#64;&#64;       (i.e. inference requests with the same correlation ID) are sent
     *&#64;&#64;       to the dynamic batcher to ensure that the sequence responses are
     *&#64;&#64;       in the correct order.
     *&#64;&#64;
     *&#64;&#64;       When using decoupled models, setting this to true may block the
     *&#64;&#64;       responses from independent sequences from being returned to the
     *&#64;&#64;       client until the previous request completes, hurting overall
     *&#64;&#64;       performance. If using GRPC streaming protocol, the stream
     *&#64;&#64;       ordering guarantee may be sufficient alone to ensure the
     *&#64;&#64;       responses for each sequence are returned in sequence-order
     *&#64;&#64;       without blocking based on independent requests, depending on the
     *&#64;&#64;       use case.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>bool preserve_ordering = 4;</code>
     * @param bool $var
     * @return $this
     */
    public function setPreserveOrdering($var)
    {
        GPBUtil::checkBool($var);
        $this->preserve_ordering = $var;

        return $this;
    }

}

// Adding a class alias for backwards compatibility with the previous class name.
class_alias(StrategyOldest::class, \Inference\ModelSequenceBatching_StrategyOldest::class);

