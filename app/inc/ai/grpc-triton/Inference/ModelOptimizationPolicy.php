<?php
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: model_config.proto

namespace Inference;

use Google\Protobuf\Internal\GPBType;
use Google\Protobuf\Internal\RepeatedField;
use Google\Protobuf\Internal\GPBUtil;

/**
 *&#64;&#64;
 *&#64;&#64;.. cpp:var:: message ModelOptimizationPolicy
 *&#64;&#64;
 *&#64;&#64;   Optimization settings for a model. These settings control if/how a
 *&#64;&#64;   model is optimized and prioritized by the backend framework when
 *&#64;&#64;   it is loaded.
 *&#64;&#64;
 *
 * Generated from protobuf message <code>inference.ModelOptimizationPolicy</code>
 */
class ModelOptimizationPolicy extends \Google\Protobuf\Internal\Message
{
    /**
     *&#64;&#64;  .. cpp:var:: Graph graph
     *&#64;&#64;
     *&#64;&#64;     The graph optimization setting for the model. Optional.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>.inference.ModelOptimizationPolicy.Graph graph = 1;</code>
     */
    protected $graph = null;
    /**
     *&#64;&#64;  .. cpp:var:: ModelPriority priority
     *&#64;&#64;
     *&#64;&#64;     The priority setting for the model. Optional.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>.inference.ModelOptimizationPolicy.ModelPriority priority = 2;</code>
     */
    protected $priority = 0;
    /**
     *&#64;&#64;  .. cpp:var:: Cuda cuda
     *&#64;&#64;
     *&#64;&#64;     CUDA-specific optimization settings. Optional.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>.inference.ModelOptimizationPolicy.Cuda cuda = 3;</code>
     */
    protected $cuda = null;
    /**
     *&#64;&#64;  .. cpp:var:: ExecutionAccelerators execution_accelerators
     *&#64;&#64;
     *&#64;&#64;     The accelerators used for the model. Optional.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>.inference.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;</code>
     */
    protected $execution_accelerators = null;
    /**
     *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer input_pinned_memory
     *&#64;&#64;
     *&#64;&#64;     Use pinned memory buffer when the data transfer for inputs
     *&#64;&#64;     is between GPU memory and non-pinned system memory.
     *&#64;&#64;     Default is true.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer input_pinned_memory = 5;</code>
     */
    protected $input_pinned_memory = null;
    /**
     *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer output_pinned_memory
     *&#64;&#64;
     *&#64;&#64;     Use pinned memory buffer when the data transfer for outputs
     *&#64;&#64;     is between GPU memory and non-pinned system memory.
     *&#64;&#64;     Default is true.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer output_pinned_memory = 6;</code>
     */
    protected $output_pinned_memory = null;
    /**
     *&#64;&#64;  .. cpp:var:: uint32 gather_kernel_buffer_threshold
     *&#64;&#64;
     *&#64;&#64;     The backend may use a gather kernel to gather input data if the
     *&#64;&#64;     device has direct access to the source buffer and the destination
     *&#64;&#64;     buffer. In such case, the gather kernel will be used only if the
     *&#64;&#64;     number of buffers to be gathered is greater or equal to
     *&#64;&#64;     the specified value. If 0, the gather kernel will be disabled.
     *&#64;&#64;     Default value is 0.
     *&#64;&#64;     Currently only recognized by TensorRT backend.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>uint32 gather_kernel_buffer_threshold = 7;</code>
     */
    protected $gather_kernel_buffer_threshold = 0;
    /**
     *&#64;&#64;  .. cpp:var:: bool eager_batching
     *&#64;&#64;
     *&#64;&#64;     Start preparing the next batch before the model instance is ready
     *&#64;&#64;     for the next inference. This option can be used to overlap the
     *&#64;&#64;     batch preparation with model execution, with the trade-off that
     *&#64;&#64;     the next batch might be smaller than what it could have been.
     *&#64;&#64;     Default value is false.
     *&#64;&#64;     Currently only recognized by TensorRT backend.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>bool eager_batching = 8;</code>
     */
    protected $eager_batching = false;

    /**
     * Constructor.
     *
     * @param array $data {
     *     Optional. Data for populating the Message object.
     *
     *     @type \Inference\ModelOptimizationPolicy\Graph $graph
     *          &#64;&#64;  .. cpp:var:: Graph graph
     *          &#64;&#64;
     *          &#64;&#64;     The graph optimization setting for the model. Optional.
     *          &#64;&#64;
     *     @type int $priority
     *          &#64;&#64;  .. cpp:var:: ModelPriority priority
     *          &#64;&#64;
     *          &#64;&#64;     The priority setting for the model. Optional.
     *          &#64;&#64;
     *     @type \Inference\ModelOptimizationPolicy\Cuda $cuda
     *          &#64;&#64;  .. cpp:var:: Cuda cuda
     *          &#64;&#64;
     *          &#64;&#64;     CUDA-specific optimization settings. Optional.
     *          &#64;&#64;
     *     @type \Inference\ModelOptimizationPolicy\ExecutionAccelerators $execution_accelerators
     *          &#64;&#64;  .. cpp:var:: ExecutionAccelerators execution_accelerators
     *          &#64;&#64;
     *          &#64;&#64;     The accelerators used for the model. Optional.
     *          &#64;&#64;
     *     @type \Inference\ModelOptimizationPolicy\PinnedMemoryBuffer $input_pinned_memory
     *          &#64;&#64;  .. cpp:var:: PinnedMemoryBuffer input_pinned_memory
     *          &#64;&#64;
     *          &#64;&#64;     Use pinned memory buffer when the data transfer for inputs
     *          &#64;&#64;     is between GPU memory and non-pinned system memory.
     *          &#64;&#64;     Default is true.
     *          &#64;&#64;
     *     @type \Inference\ModelOptimizationPolicy\PinnedMemoryBuffer $output_pinned_memory
     *          &#64;&#64;  .. cpp:var:: PinnedMemoryBuffer output_pinned_memory
     *          &#64;&#64;
     *          &#64;&#64;     Use pinned memory buffer when the data transfer for outputs
     *          &#64;&#64;     is between GPU memory and non-pinned system memory.
     *          &#64;&#64;     Default is true.
     *          &#64;&#64;
     *     @type int $gather_kernel_buffer_threshold
     *          &#64;&#64;  .. cpp:var:: uint32 gather_kernel_buffer_threshold
     *          &#64;&#64;
     *          &#64;&#64;     The backend may use a gather kernel to gather input data if the
     *          &#64;&#64;     device has direct access to the source buffer and the destination
     *          &#64;&#64;     buffer. In such case, the gather kernel will be used only if the
     *          &#64;&#64;     number of buffers to be gathered is greater or equal to
     *          &#64;&#64;     the specified value. If 0, the gather kernel will be disabled.
     *          &#64;&#64;     Default value is 0.
     *          &#64;&#64;     Currently only recognized by TensorRT backend.
     *          &#64;&#64;
     *     @type bool $eager_batching
     *          &#64;&#64;  .. cpp:var:: bool eager_batching
     *          &#64;&#64;
     *          &#64;&#64;     Start preparing the next batch before the model instance is ready
     *          &#64;&#64;     for the next inference. This option can be used to overlap the
     *          &#64;&#64;     batch preparation with model execution, with the trade-off that
     *          &#64;&#64;     the next batch might be smaller than what it could have been.
     *          &#64;&#64;     Default value is false.
     *          &#64;&#64;     Currently only recognized by TensorRT backend.
     *          &#64;&#64;
     * }
     */
    public function __construct($data = NULL) {
        \GPBMetadata\ModelConfig::initOnce();
        parent::__construct($data);
    }

    /**
     *&#64;&#64;  .. cpp:var:: Graph graph
     *&#64;&#64;
     *&#64;&#64;     The graph optimization setting for the model. Optional.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>.inference.ModelOptimizationPolicy.Graph graph = 1;</code>
     * @return \Inference\ModelOptimizationPolicy\Graph
     */
    public function getGraph()
    {
        return $this->graph;
    }

    /**
     *&#64;&#64;  .. cpp:var:: Graph graph
     *&#64;&#64;
     *&#64;&#64;     The graph optimization setting for the model. Optional.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>.inference.ModelOptimizationPolicy.Graph graph = 1;</code>
     * @param \Inference\ModelOptimizationPolicy\Graph $var
     * @return $this
     */
    public function setGraph($var)
    {
        GPBUtil::checkMessage($var, \Inference\ModelOptimizationPolicy_Graph::class);
        $this->graph = $var;

        return $this;
    }

    /**
     *&#64;&#64;  .. cpp:var:: ModelPriority priority
     *&#64;&#64;
     *&#64;&#64;     The priority setting for the model. Optional.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>.inference.ModelOptimizationPolicy.ModelPriority priority = 2;</code>
     * @return int
     */
    public function getPriority()
    {
        return $this->priority;
    }

    /**
     *&#64;&#64;  .. cpp:var:: ModelPriority priority
     *&#64;&#64;
     *&#64;&#64;     The priority setting for the model. Optional.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>.inference.ModelOptimizationPolicy.ModelPriority priority = 2;</code>
     * @param int $var
     * @return $this
     */
    public function setPriority($var)
    {
        GPBUtil::checkEnum($var, \Inference\ModelOptimizationPolicy_ModelPriority::class);
        $this->priority = $var;

        return $this;
    }

    /**
     *&#64;&#64;  .. cpp:var:: Cuda cuda
     *&#64;&#64;
     *&#64;&#64;     CUDA-specific optimization settings. Optional.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>.inference.ModelOptimizationPolicy.Cuda cuda = 3;</code>
     * @return \Inference\ModelOptimizationPolicy\Cuda
     */
    public function getCuda()
    {
        return $this->cuda;
    }

    /**
     *&#64;&#64;  .. cpp:var:: Cuda cuda
     *&#64;&#64;
     *&#64;&#64;     CUDA-specific optimization settings. Optional.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>.inference.ModelOptimizationPolicy.Cuda cuda = 3;</code>
     * @param \Inference\ModelOptimizationPolicy\Cuda $var
     * @return $this
     */
    public function setCuda($var)
    {
        GPBUtil::checkMessage($var, \Inference\ModelOptimizationPolicy_Cuda::class);
        $this->cuda = $var;

        return $this;
    }

    /**
     *&#64;&#64;  .. cpp:var:: ExecutionAccelerators execution_accelerators
     *&#64;&#64;
     *&#64;&#64;     The accelerators used for the model. Optional.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>.inference.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;</code>
     * @return \Inference\ModelOptimizationPolicy\ExecutionAccelerators
     */
    public function getExecutionAccelerators()
    {
        return $this->execution_accelerators;
    }

    /**
     *&#64;&#64;  .. cpp:var:: ExecutionAccelerators execution_accelerators
     *&#64;&#64;
     *&#64;&#64;     The accelerators used for the model. Optional.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>.inference.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;</code>
     * @param \Inference\ModelOptimizationPolicy\ExecutionAccelerators $var
     * @return $this
     */
    public function setExecutionAccelerators($var)
    {
        GPBUtil::checkMessage($var, \Inference\ModelOptimizationPolicy_ExecutionAccelerators::class);
        $this->execution_accelerators = $var;

        return $this;
    }

    /**
     *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer input_pinned_memory
     *&#64;&#64;
     *&#64;&#64;     Use pinned memory buffer when the data transfer for inputs
     *&#64;&#64;     is between GPU memory and non-pinned system memory.
     *&#64;&#64;     Default is true.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer input_pinned_memory = 5;</code>
     * @return \Inference\ModelOptimizationPolicy\PinnedMemoryBuffer
     */
    public function getInputPinnedMemory()
    {
        return $this->input_pinned_memory;
    }

    /**
     *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer input_pinned_memory
     *&#64;&#64;
     *&#64;&#64;     Use pinned memory buffer when the data transfer for inputs
     *&#64;&#64;     is between GPU memory and non-pinned system memory.
     *&#64;&#64;     Default is true.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer input_pinned_memory = 5;</code>
     * @param \Inference\ModelOptimizationPolicy\PinnedMemoryBuffer $var
     * @return $this
     */
    public function setInputPinnedMemory($var)
    {
        GPBUtil::checkMessage($var, \Inference\ModelOptimizationPolicy_PinnedMemoryBuffer::class);
        $this->input_pinned_memory = $var;

        return $this;
    }

    /**
     *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer output_pinned_memory
     *&#64;&#64;
     *&#64;&#64;     Use pinned memory buffer when the data transfer for outputs
     *&#64;&#64;     is between GPU memory and non-pinned system memory.
     *&#64;&#64;     Default is true.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer output_pinned_memory = 6;</code>
     * @return \Inference\ModelOptimizationPolicy\PinnedMemoryBuffer
     */
    public function getOutputPinnedMemory()
    {
        return $this->output_pinned_memory;
    }

    /**
     *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer output_pinned_memory
     *&#64;&#64;
     *&#64;&#64;     Use pinned memory buffer when the data transfer for outputs
     *&#64;&#64;     is between GPU memory and non-pinned system memory.
     *&#64;&#64;     Default is true.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer output_pinned_memory = 6;</code>
     * @param \Inference\ModelOptimizationPolicy\PinnedMemoryBuffer $var
     * @return $this
     */
    public function setOutputPinnedMemory($var)
    {
        GPBUtil::checkMessage($var, \Inference\ModelOptimizationPolicy_PinnedMemoryBuffer::class);
        $this->output_pinned_memory = $var;

        return $this;
    }

    /**
     *&#64;&#64;  .. cpp:var:: uint32 gather_kernel_buffer_threshold
     *&#64;&#64;
     *&#64;&#64;     The backend may use a gather kernel to gather input data if the
     *&#64;&#64;     device has direct access to the source buffer and the destination
     *&#64;&#64;     buffer. In such case, the gather kernel will be used only if the
     *&#64;&#64;     number of buffers to be gathered is greater or equal to
     *&#64;&#64;     the specified value. If 0, the gather kernel will be disabled.
     *&#64;&#64;     Default value is 0.
     *&#64;&#64;     Currently only recognized by TensorRT backend.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>uint32 gather_kernel_buffer_threshold = 7;</code>
     * @return int
     */
    public function getGatherKernelBufferThreshold()
    {
        return $this->gather_kernel_buffer_threshold;
    }

    /**
     *&#64;&#64;  .. cpp:var:: uint32 gather_kernel_buffer_threshold
     *&#64;&#64;
     *&#64;&#64;     The backend may use a gather kernel to gather input data if the
     *&#64;&#64;     device has direct access to the source buffer and the destination
     *&#64;&#64;     buffer. In such case, the gather kernel will be used only if the
     *&#64;&#64;     number of buffers to be gathered is greater or equal to
     *&#64;&#64;     the specified value. If 0, the gather kernel will be disabled.
     *&#64;&#64;     Default value is 0.
     *&#64;&#64;     Currently only recognized by TensorRT backend.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>uint32 gather_kernel_buffer_threshold = 7;</code>
     * @param int $var
     * @return $this
     */
    public function setGatherKernelBufferThreshold($var)
    {
        GPBUtil::checkUint32($var);
        $this->gather_kernel_buffer_threshold = $var;

        return $this;
    }

    /**
     *&#64;&#64;  .. cpp:var:: bool eager_batching
     *&#64;&#64;
     *&#64;&#64;     Start preparing the next batch before the model instance is ready
     *&#64;&#64;     for the next inference. This option can be used to overlap the
     *&#64;&#64;     batch preparation with model execution, with the trade-off that
     *&#64;&#64;     the next batch might be smaller than what it could have been.
     *&#64;&#64;     Default value is false.
     *&#64;&#64;     Currently only recognized by TensorRT backend.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>bool eager_batching = 8;</code>
     * @return bool
     */
    public function getEagerBatching()
    {
        return $this->eager_batching;
    }

    /**
     *&#64;&#64;  .. cpp:var:: bool eager_batching
     *&#64;&#64;
     *&#64;&#64;     Start preparing the next batch before the model instance is ready
     *&#64;&#64;     for the next inference. This option can be used to overlap the
     *&#64;&#64;     batch preparation with model execution, with the trade-off that
     *&#64;&#64;     the next batch might be smaller than what it could have been.
     *&#64;&#64;     Default value is false.
     *&#64;&#64;     Currently only recognized by TensorRT backend.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>bool eager_batching = 8;</code>
     * @param bool $var
     * @return $this
     */
    public function setEagerBatching($var)
    {
        GPBUtil::checkBool($var);
        $this->eager_batching = $var;

        return $this;
    }

}

