<?php
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: model_config.proto

namespace Inference\ModelOptimizationPolicy;

use Google\Protobuf\Internal\GPBType;
use Google\Protobuf\Internal\RepeatedField;
use Google\Protobuf\Internal\GPBUtil;

/**
 *&#64;&#64;
 *&#64;&#64;  .. cpp:var:: message Cuda
 *&#64;&#64;
 *&#64;&#64;     CUDA-specific optimization settings.
 *&#64;&#64;
 *
 * Generated from protobuf message <code>inference.ModelOptimizationPolicy.Cuda</code>
 */
class Cuda extends \Google\Protobuf\Internal\Message
{
    /**
     *&#64;&#64;    .. cpp:var:: bool graphs
     *&#64;&#64;
     *&#64;&#64;       Use CUDA graphs API to capture model operations and execute
     *&#64;&#64;       them more efficiently. Default value is false.
     *&#64;&#64;       Currently only recognized by TensorRT backend.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>bool graphs = 1;</code>
     */
    protected $graphs = false;
    /**
     *&#64;&#64;    .. cpp:var:: bool busy_wait_events
     *&#64;&#64;
     *&#64;&#64;       Use busy-waiting to synchronize CUDA events to achieve minimum
     *&#64;&#64;       latency from event complete to host thread to be notified, with
     *&#64;&#64;       the cost of high CPU load. Default value is false.
     *&#64;&#64;       Currently only recognized by TensorRT backend.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>bool busy_wait_events = 2;</code>
     */
    protected $busy_wait_events = false;
    /**
     *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
     *&#64;&#64;
     *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
     *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
     *&#64;&#64;       based on model settings.
     *&#64;&#64;       Currently only recognized by TensorRT backend.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
     */
    private $graph_spec;
    /**
     *&#64;&#64;    .. cpp:var:: bool output_copy_stream
     *&#64;&#64;
     *&#64;&#64;       Uses a CUDA stream separate from the inference stream to copy the
     *&#64;&#64;       output to host. However, be aware that setting this option to
     *&#64;&#64;       true will lead to an increase in the memory consumption of the
     *&#64;&#64;       model as Triton will allocate twice as much GPU memory for its
     *&#64;&#64;       I/O tensor buffers. Default value is false.
     *&#64;&#64;       Currently only recognized by TensorRT backend.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>bool output_copy_stream = 4;</code>
     */
    protected $output_copy_stream = false;

    /**
     * Constructor.
     *
     * @param array $data {
     *     Optional. Data for populating the Message object.
     *
     *     @type bool $graphs
     *          &#64;&#64;    .. cpp:var:: bool graphs
     *          &#64;&#64;
     *          &#64;&#64;       Use CUDA graphs API to capture model operations and execute
     *          &#64;&#64;       them more efficiently. Default value is false.
     *          &#64;&#64;       Currently only recognized by TensorRT backend.
     *          &#64;&#64;
     *     @type bool $busy_wait_events
     *          &#64;&#64;    .. cpp:var:: bool busy_wait_events
     *          &#64;&#64;
     *          &#64;&#64;       Use busy-waiting to synchronize CUDA events to achieve minimum
     *          &#64;&#64;       latency from event complete to host thread to be notified, with
     *          &#64;&#64;       the cost of high CPU load. Default value is false.
     *          &#64;&#64;       Currently only recognized by TensorRT backend.
     *          &#64;&#64;
     *     @type \Inference\ModelOptimizationPolicy\Cuda\GraphSpec[]|\Google\Protobuf\Internal\RepeatedField $graph_spec
     *          &#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
     *          &#64;&#64;
     *          &#64;&#64;       Specification of the CUDA graph to be captured. If not specified
     *          &#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
     *          &#64;&#64;       based on model settings.
     *          &#64;&#64;       Currently only recognized by TensorRT backend.
     *          &#64;&#64;
     *     @type bool $output_copy_stream
     *          &#64;&#64;    .. cpp:var:: bool output_copy_stream
     *          &#64;&#64;
     *          &#64;&#64;       Uses a CUDA stream separate from the inference stream to copy the
     *          &#64;&#64;       output to host. However, be aware that setting this option to
     *          &#64;&#64;       true will lead to an increase in the memory consumption of the
     *          &#64;&#64;       model as Triton will allocate twice as much GPU memory for its
     *          &#64;&#64;       I/O tensor buffers. Default value is false.
     *          &#64;&#64;       Currently only recognized by TensorRT backend.
     *          &#64;&#64;
     * }
     */
    public function __construct($data = NULL) {
        \GPBMetadata\ModelConfig::initOnce();
        parent::__construct($data);
    }

    /**
     *&#64;&#64;    .. cpp:var:: bool graphs
     *&#64;&#64;
     *&#64;&#64;       Use CUDA graphs API to capture model operations and execute
     *&#64;&#64;       them more efficiently. Default value is false.
     *&#64;&#64;       Currently only recognized by TensorRT backend.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>bool graphs = 1;</code>
     * @return bool
     */
    public function getGraphs()
    {
        return $this->graphs;
    }

    /**
     *&#64;&#64;    .. cpp:var:: bool graphs
     *&#64;&#64;
     *&#64;&#64;       Use CUDA graphs API to capture model operations and execute
     *&#64;&#64;       them more efficiently. Default value is false.
     *&#64;&#64;       Currently only recognized by TensorRT backend.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>bool graphs = 1;</code>
     * @param bool $var
     * @return $this
     */
    public function setGraphs($var)
    {
        GPBUtil::checkBool($var);
        $this->graphs = $var;

        return $this;
    }

    /**
     *&#64;&#64;    .. cpp:var:: bool busy_wait_events
     *&#64;&#64;
     *&#64;&#64;       Use busy-waiting to synchronize CUDA events to achieve minimum
     *&#64;&#64;       latency from event complete to host thread to be notified, with
     *&#64;&#64;       the cost of high CPU load. Default value is false.
     *&#64;&#64;       Currently only recognized by TensorRT backend.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>bool busy_wait_events = 2;</code>
     * @return bool
     */
    public function getBusyWaitEvents()
    {
        return $this->busy_wait_events;
    }

    /**
     *&#64;&#64;    .. cpp:var:: bool busy_wait_events
     *&#64;&#64;
     *&#64;&#64;       Use busy-waiting to synchronize CUDA events to achieve minimum
     *&#64;&#64;       latency from event complete to host thread to be notified, with
     *&#64;&#64;       the cost of high CPU load. Default value is false.
     *&#64;&#64;       Currently only recognized by TensorRT backend.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>bool busy_wait_events = 2;</code>
     * @param bool $var
     * @return $this
     */
    public function setBusyWaitEvents($var)
    {
        GPBUtil::checkBool($var);
        $this->busy_wait_events = $var;

        return $this;
    }

    /**
     *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
     *&#64;&#64;
     *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
     *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
     *&#64;&#64;       based on model settings.
     *&#64;&#64;       Currently only recognized by TensorRT backend.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
     * @return \Google\Protobuf\Internal\RepeatedField
     */
    public function getGraphSpec()
    {
        return $this->graph_spec;
    }

    /**
     *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
     *&#64;&#64;
     *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
     *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
     *&#64;&#64;       based on model settings.
     *&#64;&#64;       Currently only recognized by TensorRT backend.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
     * @param \Inference\ModelOptimizationPolicy\Cuda\GraphSpec[]|\Google\Protobuf\Internal\RepeatedField $var
     * @return $this
     */
    public function setGraphSpec($var)
    {
        $arr = GPBUtil::checkRepeatedField($var, \Google\Protobuf\Internal\GPBType::MESSAGE, \Inference\ModelOptimizationPolicy\Cuda\GraphSpec::class);
        $this->graph_spec = $arr;

        return $this;
    }

    /**
     *&#64;&#64;    .. cpp:var:: bool output_copy_stream
     *&#64;&#64;
     *&#64;&#64;       Uses a CUDA stream separate from the inference stream to copy the
     *&#64;&#64;       output to host. However, be aware that setting this option to
     *&#64;&#64;       true will lead to an increase in the memory consumption of the
     *&#64;&#64;       model as Triton will allocate twice as much GPU memory for its
     *&#64;&#64;       I/O tensor buffers. Default value is false.
     *&#64;&#64;       Currently only recognized by TensorRT backend.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>bool output_copy_stream = 4;</code>
     * @return bool
     */
    public function getOutputCopyStream()
    {
        return $this->output_copy_stream;
    }

    /**
     *&#64;&#64;    .. cpp:var:: bool output_copy_stream
     *&#64;&#64;
     *&#64;&#64;       Uses a CUDA stream separate from the inference stream to copy the
     *&#64;&#64;       output to host. However, be aware that setting this option to
     *&#64;&#64;       true will lead to an increase in the memory consumption of the
     *&#64;&#64;       model as Triton will allocate twice as much GPU memory for its
     *&#64;&#64;       I/O tensor buffers. Default value is false.
     *&#64;&#64;       Currently only recognized by TensorRT backend.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>bool output_copy_stream = 4;</code>
     * @param bool $var
     * @return $this
     */
    public function setOutputCopyStream($var)
    {
        GPBUtil::checkBool($var);
        $this->output_copy_stream = $var;

        return $this;
    }

}

// Adding a class alias for backwards compatibility with the previous class name.
class_alias(Cuda::class, \Inference\ModelOptimizationPolicy_Cuda::class);

