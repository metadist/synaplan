<?php
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: model_config.proto

namespace Inference\ModelOptimizationPolicy;

use Google\Protobuf\Internal\GPBType;
use Google\Protobuf\Internal\RepeatedField;
use Google\Protobuf\Internal\GPBUtil;

/**
 *&#64;&#64;
 *&#64;&#64;  .. cpp:var:: message ExecutionAccelerators
 *&#64;&#64;
 *&#64;&#64;     Specify the preferred execution accelerators to be used to execute
 *&#64;&#64;     the model. Currently only recognized by ONNX Runtime backend and
 *&#64;&#64;     TensorFlow backend.
 *&#64;&#64;
 *&#64;&#64;     For ONNX Runtime backend, it will deploy the model with the execution
 *&#64;&#64;     accelerators by priority, the priority is determined based on the
 *&#64;&#64;     order that they are set, i.e. the provider at the front has highest
 *&#64;&#64;     priority. Overall, the priority will be in the following order:
 *&#64;&#64;         <gpu_execution_accelerator> (if instance is on GPU)
 *&#64;&#64;         CUDA Execution Provider     (if instance is on GPU)
 *&#64;&#64;         <cpu_execution_accelerator>
 *&#64;&#64;         Default CPU Execution Provider
 *&#64;&#64;
 *
 * Generated from protobuf message <code>inference.ModelOptimizationPolicy.ExecutionAccelerators</code>
 */
class ExecutionAccelerators extends \Google\Protobuf\Internal\Message
{
    /**
     *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
     *&#64;&#64;
     *&#64;&#64;       The preferred execution provider to be used if the model instance
     *&#64;&#64;       is deployed on GPU.
     *&#64;&#64;
     *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
     *&#64;&#64;       and no parameters are required.
     *&#64;&#64;
     *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
     *&#64;&#64;       "auto_mixed_precision", "gpu_io".
     *&#64;&#64;
     *&#64;&#64;       For "tensorrt", the following parameters can be specified:
     *&#64;&#64;         "precision_mode": The precision used for optimization.
     *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
     *&#64;&#64;
     *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
     *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
     *&#64;&#64;
     *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
     *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
     *&#64;&#64;
     *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
     *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
     *&#64;&#64;
     *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
     *&#64;&#64;       the model will try to use FP16 for better performance.
     *&#64;&#64;       This optimization can not be set with "tensorrt".
     *&#64;&#64;
     *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
     *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
     *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
     *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
     *&#64;&#64;       object will be created on model creation and it will request all
     *&#64;&#64;       outputs for every model execution, which may impact the
     *&#64;&#64;       performance if a request does not require all outputs. This
     *&#64;&#64;       optimization will only take affect if the model instance is
     *&#64;&#64;       created with KIND_GPU.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
     */
    private $gpu_execution_accelerator;
    /**
     *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
     *&#64;&#64;
     *&#64;&#64;       The preferred execution provider to be used if the model instance
     *&#64;&#64;       is deployed on CPU.
     *&#64;&#64;
     *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
     *&#64;&#64;       and no parameters are required.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
     */
    private $cpu_execution_accelerator;

    /**
     * Constructor.
     *
     * @param array $data {
     *     Optional. Data for populating the Message object.
     *
     *     @type \Inference\ModelOptimizationPolicy\ExecutionAccelerators\Accelerator[]|\Google\Protobuf\Internal\RepeatedField $gpu_execution_accelerator
     *          &#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
     *          &#64;&#64;
     *          &#64;&#64;       The preferred execution provider to be used if the model instance
     *          &#64;&#64;       is deployed on GPU.
     *          &#64;&#64;
     *          &#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
     *          &#64;&#64;       and no parameters are required.
     *          &#64;&#64;
     *          &#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
     *          &#64;&#64;       "auto_mixed_precision", "gpu_io".
     *          &#64;&#64;
     *          &#64;&#64;       For "tensorrt", the following parameters can be specified:
     *          &#64;&#64;         "precision_mode": The precision used for optimization.
     *          &#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
     *          &#64;&#64;
     *          &#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
     *          &#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
     *          &#64;&#64;
     *          &#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
     *          &#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
     *          &#64;&#64;
     *          &#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
     *          &#64;&#64;         can use temporarily during execution. Default value is 1GB.
     *          &#64;&#64;
     *          &#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
     *          &#64;&#64;       the model will try to use FP16 for better performance.
     *          &#64;&#64;       This optimization can not be set with "tensorrt".
     *          &#64;&#64;
     *          &#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
     *          &#64;&#64;       be executed using TensorFlow Callable API to set input and output
     *          &#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
     *          &#64;&#64;       overhead if the model is used in ensemble. However, the Callable
     *          &#64;&#64;       object will be created on model creation and it will request all
     *          &#64;&#64;       outputs for every model execution, which may impact the
     *          &#64;&#64;       performance if a request does not require all outputs. This
     *          &#64;&#64;       optimization will only take affect if the model instance is
     *          &#64;&#64;       created with KIND_GPU.
     *          &#64;&#64;
     *     @type \Inference\ModelOptimizationPolicy\ExecutionAccelerators\Accelerator[]|\Google\Protobuf\Internal\RepeatedField $cpu_execution_accelerator
     *          &#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
     *          &#64;&#64;
     *          &#64;&#64;       The preferred execution provider to be used if the model instance
     *          &#64;&#64;       is deployed on CPU.
     *          &#64;&#64;
     *          &#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
     *          &#64;&#64;       and no parameters are required.
     *          &#64;&#64;
     * }
     */
    public function __construct($data = NULL) {
        \GPBMetadata\ModelConfig::initOnce();
        parent::__construct($data);
    }

    /**
     *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
     *&#64;&#64;
     *&#64;&#64;       The preferred execution provider to be used if the model instance
     *&#64;&#64;       is deployed on GPU.
     *&#64;&#64;
     *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
     *&#64;&#64;       and no parameters are required.
     *&#64;&#64;
     *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
     *&#64;&#64;       "auto_mixed_precision", "gpu_io".
     *&#64;&#64;
     *&#64;&#64;       For "tensorrt", the following parameters can be specified:
     *&#64;&#64;         "precision_mode": The precision used for optimization.
     *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
     *&#64;&#64;
     *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
     *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
     *&#64;&#64;
     *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
     *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
     *&#64;&#64;
     *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
     *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
     *&#64;&#64;
     *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
     *&#64;&#64;       the model will try to use FP16 for better performance.
     *&#64;&#64;       This optimization can not be set with "tensorrt".
     *&#64;&#64;
     *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
     *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
     *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
     *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
     *&#64;&#64;       object will be created on model creation and it will request all
     *&#64;&#64;       outputs for every model execution, which may impact the
     *&#64;&#64;       performance if a request does not require all outputs. This
     *&#64;&#64;       optimization will only take affect if the model instance is
     *&#64;&#64;       created with KIND_GPU.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
     * @return \Google\Protobuf\Internal\RepeatedField
     */
    public function getGpuExecutionAccelerator()
    {
        return $this->gpu_execution_accelerator;
    }

    /**
     *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
     *&#64;&#64;
     *&#64;&#64;       The preferred execution provider to be used if the model instance
     *&#64;&#64;       is deployed on GPU.
     *&#64;&#64;
     *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
     *&#64;&#64;       and no parameters are required.
     *&#64;&#64;
     *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
     *&#64;&#64;       "auto_mixed_precision", "gpu_io".
     *&#64;&#64;
     *&#64;&#64;       For "tensorrt", the following parameters can be specified:
     *&#64;&#64;         "precision_mode": The precision used for optimization.
     *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
     *&#64;&#64;
     *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
     *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
     *&#64;&#64;
     *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
     *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
     *&#64;&#64;
     *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
     *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
     *&#64;&#64;
     *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
     *&#64;&#64;       the model will try to use FP16 for better performance.
     *&#64;&#64;       This optimization can not be set with "tensorrt".
     *&#64;&#64;
     *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
     *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
     *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
     *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
     *&#64;&#64;       object will be created on model creation and it will request all
     *&#64;&#64;       outputs for every model execution, which may impact the
     *&#64;&#64;       performance if a request does not require all outputs. This
     *&#64;&#64;       optimization will only take affect if the model instance is
     *&#64;&#64;       created with KIND_GPU.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
     * @param \Inference\ModelOptimizationPolicy\ExecutionAccelerators\Accelerator[]|\Google\Protobuf\Internal\RepeatedField $var
     * @return $this
     */
    public function setGpuExecutionAccelerator($var)
    {
        $arr = GPBUtil::checkRepeatedField($var, \Google\Protobuf\Internal\GPBType::MESSAGE, \Inference\ModelOptimizationPolicy\ExecutionAccelerators\Accelerator::class);
        $this->gpu_execution_accelerator = $arr;

        return $this;
    }

    /**
     *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
     *&#64;&#64;
     *&#64;&#64;       The preferred execution provider to be used if the model instance
     *&#64;&#64;       is deployed on CPU.
     *&#64;&#64;
     *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
     *&#64;&#64;       and no parameters are required.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
     * @return \Google\Protobuf\Internal\RepeatedField
     */
    public function getCpuExecutionAccelerator()
    {
        return $this->cpu_execution_accelerator;
    }

    /**
     *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
     *&#64;&#64;
     *&#64;&#64;       The preferred execution provider to be used if the model instance
     *&#64;&#64;       is deployed on CPU.
     *&#64;&#64;
     *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
     *&#64;&#64;       and no parameters are required.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
     * @param \Inference\ModelOptimizationPolicy\ExecutionAccelerators\Accelerator[]|\Google\Protobuf\Internal\RepeatedField $var
     * @return $this
     */
    public function setCpuExecutionAccelerator($var)
    {
        $arr = GPBUtil::checkRepeatedField($var, \Google\Protobuf\Internal\GPBType::MESSAGE, \Inference\ModelOptimizationPolicy\ExecutionAccelerators\Accelerator::class);
        $this->cpu_execution_accelerator = $arr;

        return $this;
    }

}

// Adding a class alias for backwards compatibility with the previous class name.
class_alias(ExecutionAccelerators::class, \Inference\ModelOptimizationPolicy_ExecutionAccelerators::class);

