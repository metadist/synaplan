<?php
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: grpc_service.proto

namespace Inference;

use Google\Protobuf\Internal\GPBType;
use Google\Protobuf\Internal\RepeatedField;
use Google\Protobuf\Internal\GPBUtil;

/**
 *&#64;&#64;
 *&#64;&#64;.. cpp:var:: message ModelStatistics
 *&#64;&#64;
 *&#64;&#64;   Statistics for a specific model and version.
 *&#64;&#64;
 *
 * Generated from protobuf message <code>inference.ModelStatistics</code>
 */
class ModelStatistics extends \Google\Protobuf\Internal\Message
{
    /**
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the model. If not given returns statistics for all
     *&#64;&#64;
     *
     * Generated from protobuf field <code>string name = 1;</code>
     */
    protected $name = '';
    /**
     *&#64;&#64;  .. cpp:var:: string version
     *&#64;&#64;
     *&#64;&#64;     The version of the model.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>string version = 2;</code>
     */
    protected $version = '';
    /**
     *&#64;&#64;  .. cpp:var:: uint64 last_inference
     *&#64;&#64;
     *&#64;&#64;     The timestamp of the last inference request made for this model,
     *&#64;&#64;     as milliseconds since the epoch.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>uint64 last_inference = 3;</code>
     */
    protected $last_inference = 0;
    /**
     *&#64;&#64;  .. cpp:var:: uint64 last_inference
     *&#64;&#64;
     *&#64;&#64;     The cumulative count of successful inference requests made for this
     *&#64;&#64;     model. Each inference in a batched request is counted as an
     *&#64;&#64;     individual inference. For example, if a client sends a single
     *&#64;&#64;     inference request with batch size 64, "inference_count" will be
     *&#64;&#64;     incremented by 64. Similarly, if a clients sends 64 individual
     *&#64;&#64;     requests each with batch size 1, "inference_count" will be
     *&#64;&#64;     incremented by 64. The "inference_count" value DOES NOT include
     *&#64;&#64;     cache hits.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>uint64 inference_count = 4;</code>
     */
    protected $inference_count = 0;
    /**
     *&#64;&#64;  .. cpp:var:: uint64 last_inference
     *&#64;&#64;
     *&#64;&#64;     The cumulative count of the number of successful inference executions
     *&#64;&#64;     performed for the model. When dynamic batching is enabled, a single
     *&#64;&#64;     model execution can perform inferencing for more than one inference
     *&#64;&#64;     request. For example, if a clients sends 64 individual requests each
     *&#64;&#64;     with batch size 1 and the dynamic batcher batches them into a single
     *&#64;&#64;     large batch for model execution then "execution_count" will be
     *&#64;&#64;     incremented by 1. If, on the other hand, the dynamic batcher is not
     *&#64;&#64;     enabled for that each of the 64 individual requests is executed
     *&#64;&#64;     independently, then "execution_count" will be incremented by 64.
     *&#64;&#64;     The "execution_count" value DOES NOT include cache hits.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>uint64 execution_count = 5;</code>
     */
    protected $execution_count = 0;
    /**
     *&#64;&#64;  .. cpp:var:: InferStatistics inference_stats
     *&#64;&#64;
     *&#64;&#64;     The aggregate statistics for the model/version.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>.inference.InferStatistics inference_stats = 6;</code>
     */
    protected $inference_stats = null;
    /**
     *&#64;&#64;  .. cpp:var:: InferBatchStatistics batch_stats (repeated)
     *&#64;&#64;
     *&#64;&#64;     The aggregate statistics for each different batch size that is
     *&#64;&#64;     executed in the model. The batch statistics indicate how many actual
     *&#64;&#64;     model executions were performed and show differences due to different
     *&#64;&#64;     batch size (for example, larger batches typically take longer to
     *&#64;&#64;     compute).
     *&#64;&#64;
     *
     * Generated from protobuf field <code>repeated .inference.InferBatchStatistics batch_stats = 7;</code>
     */
    private $batch_stats;
    /**
     *&#64;&#64;  .. cpp:var:: MemoryUsage memory_usage (repeated)
     *&#64;&#64;
     *&#64;&#64;     The memory usage detected during model loading, which may be used to
     *&#64;&#64;     estimate the memory to be released once the model is unloaded. Note
     *&#64;&#64;     that the estimation is inferenced by the profiling tools and
     *&#64;&#64;     framework's memory schema, therefore it is advised to perform
     *&#64;&#64;     experiments to understand the scenario that the reported memory usage
     *&#64;&#64;     can be relied on. As a starting point, the GPU memory usage for
     *&#64;&#64;     models in ONNX Runtime backend and TensorRT backend is usually
     *&#64;&#64;     aligned.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>repeated .inference.MemoryUsage memory_usage = 8;</code>
     */
    private $memory_usage;
    /**
     *&#64;&#64;  .. cpp:var:: map<string, InferResponseStatistics> response_stats
     *&#64;&#64;
     *&#64;&#64;     The key and value pairs for all responses statistics. The key is a
     *&#64;&#64;     string identifying a set of response statistics aggregated together
     *&#64;&#64;     (i.e. index of the response sent). The value is the aggregated
     *&#64;&#64;     response statistics.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>map<string, .inference.InferResponseStatistics> response_stats = 9;</code>
     */
    private $response_stats;

    /**
     * Constructor.
     *
     * @param array $data {
     *     Optional. Data for populating the Message object.
     *
     *     @type string $name
     *          &#64;&#64;  .. cpp:var:: string name
     *          &#64;&#64;
     *          &#64;&#64;     The name of the model. If not given returns statistics for all
     *          &#64;&#64;
     *     @type string $version
     *          &#64;&#64;  .. cpp:var:: string version
     *          &#64;&#64;
     *          &#64;&#64;     The version of the model.
     *          &#64;&#64;
     *     @type int|string $last_inference
     *          &#64;&#64;  .. cpp:var:: uint64 last_inference
     *          &#64;&#64;
     *          &#64;&#64;     The timestamp of the last inference request made for this model,
     *          &#64;&#64;     as milliseconds since the epoch.
     *          &#64;&#64;
     *     @type int|string $inference_count
     *          &#64;&#64;  .. cpp:var:: uint64 last_inference
     *          &#64;&#64;
     *          &#64;&#64;     The cumulative count of successful inference requests made for this
     *          &#64;&#64;     model. Each inference in a batched request is counted as an
     *          &#64;&#64;     individual inference. For example, if a client sends a single
     *          &#64;&#64;     inference request with batch size 64, "inference_count" will be
     *          &#64;&#64;     incremented by 64. Similarly, if a clients sends 64 individual
     *          &#64;&#64;     requests each with batch size 1, "inference_count" will be
     *          &#64;&#64;     incremented by 64. The "inference_count" value DOES NOT include
     *          &#64;&#64;     cache hits.
     *          &#64;&#64;
     *     @type int|string $execution_count
     *          &#64;&#64;  .. cpp:var:: uint64 last_inference
     *          &#64;&#64;
     *          &#64;&#64;     The cumulative count of the number of successful inference executions
     *          &#64;&#64;     performed for the model. When dynamic batching is enabled, a single
     *          &#64;&#64;     model execution can perform inferencing for more than one inference
     *          &#64;&#64;     request. For example, if a clients sends 64 individual requests each
     *          &#64;&#64;     with batch size 1 and the dynamic batcher batches them into a single
     *          &#64;&#64;     large batch for model execution then "execution_count" will be
     *          &#64;&#64;     incremented by 1. If, on the other hand, the dynamic batcher is not
     *          &#64;&#64;     enabled for that each of the 64 individual requests is executed
     *          &#64;&#64;     independently, then "execution_count" will be incremented by 64.
     *          &#64;&#64;     The "execution_count" value DOES NOT include cache hits.
     *          &#64;&#64;
     *     @type \Inference\InferStatistics $inference_stats
     *          &#64;&#64;  .. cpp:var:: InferStatistics inference_stats
     *          &#64;&#64;
     *          &#64;&#64;     The aggregate statistics for the model/version.
     *          &#64;&#64;
     *     @type \Inference\InferBatchStatistics[]|\Google\Protobuf\Internal\RepeatedField $batch_stats
     *          &#64;&#64;  .. cpp:var:: InferBatchStatistics batch_stats (repeated)
     *          &#64;&#64;
     *          &#64;&#64;     The aggregate statistics for each different batch size that is
     *          &#64;&#64;     executed in the model. The batch statistics indicate how many actual
     *          &#64;&#64;     model executions were performed and show differences due to different
     *          &#64;&#64;     batch size (for example, larger batches typically take longer to
     *          &#64;&#64;     compute).
     *          &#64;&#64;
     *     @type \Inference\MemoryUsage[]|\Google\Protobuf\Internal\RepeatedField $memory_usage
     *          &#64;&#64;  .. cpp:var:: MemoryUsage memory_usage (repeated)
     *          &#64;&#64;
     *          &#64;&#64;     The memory usage detected during model loading, which may be used to
     *          &#64;&#64;     estimate the memory to be released once the model is unloaded. Note
     *          &#64;&#64;     that the estimation is inferenced by the profiling tools and
     *          &#64;&#64;     framework's memory schema, therefore it is advised to perform
     *          &#64;&#64;     experiments to understand the scenario that the reported memory usage
     *          &#64;&#64;     can be relied on. As a starting point, the GPU memory usage for
     *          &#64;&#64;     models in ONNX Runtime backend and TensorRT backend is usually
     *          &#64;&#64;     aligned.
     *          &#64;&#64;
     *     @type array|\Google\Protobuf\Internal\MapField $response_stats
     *          &#64;&#64;  .. cpp:var:: map<string, InferResponseStatistics> response_stats
     *          &#64;&#64;
     *          &#64;&#64;     The key and value pairs for all responses statistics. The key is a
     *          &#64;&#64;     string identifying a set of response statistics aggregated together
     *          &#64;&#64;     (i.e. index of the response sent). The value is the aggregated
     *          &#64;&#64;     response statistics.
     *          &#64;&#64;
     * }
     */
    public function __construct($data = NULL) {
        \GPBMetadata\GrpcService::initOnce();
        parent::__construct($data);
    }

    /**
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the model. If not given returns statistics for all
     *&#64;&#64;
     *
     * Generated from protobuf field <code>string name = 1;</code>
     * @return string
     */
    public function getName()
    {
        return $this->name;
    }

    /**
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the model. If not given returns statistics for all
     *&#64;&#64;
     *
     * Generated from protobuf field <code>string name = 1;</code>
     * @param string $var
     * @return $this
     */
    public function setName($var)
    {
        GPBUtil::checkString($var, True);
        $this->name = $var;

        return $this;
    }

    /**
     *&#64;&#64;  .. cpp:var:: string version
     *&#64;&#64;
     *&#64;&#64;     The version of the model.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>string version = 2;</code>
     * @return string
     */
    public function getVersion()
    {
        return $this->version;
    }

    /**
     *&#64;&#64;  .. cpp:var:: string version
     *&#64;&#64;
     *&#64;&#64;     The version of the model.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>string version = 2;</code>
     * @param string $var
     * @return $this
     */
    public function setVersion($var)
    {
        GPBUtil::checkString($var, True);
        $this->version = $var;

        return $this;
    }

    /**
     *&#64;&#64;  .. cpp:var:: uint64 last_inference
     *&#64;&#64;
     *&#64;&#64;     The timestamp of the last inference request made for this model,
     *&#64;&#64;     as milliseconds since the epoch.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>uint64 last_inference = 3;</code>
     * @return int|string
     */
    public function getLastInference()
    {
        return $this->last_inference;
    }

    /**
     *&#64;&#64;  .. cpp:var:: uint64 last_inference
     *&#64;&#64;
     *&#64;&#64;     The timestamp of the last inference request made for this model,
     *&#64;&#64;     as milliseconds since the epoch.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>uint64 last_inference = 3;</code>
     * @param int|string $var
     * @return $this
     */
    public function setLastInference($var)
    {
        GPBUtil::checkUint64($var);
        $this->last_inference = $var;

        return $this;
    }

    /**
     *&#64;&#64;  .. cpp:var:: uint64 last_inference
     *&#64;&#64;
     *&#64;&#64;     The cumulative count of successful inference requests made for this
     *&#64;&#64;     model. Each inference in a batched request is counted as an
     *&#64;&#64;     individual inference. For example, if a client sends a single
     *&#64;&#64;     inference request with batch size 64, "inference_count" will be
     *&#64;&#64;     incremented by 64. Similarly, if a clients sends 64 individual
     *&#64;&#64;     requests each with batch size 1, "inference_count" will be
     *&#64;&#64;     incremented by 64. The "inference_count" value DOES NOT include
     *&#64;&#64;     cache hits.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>uint64 inference_count = 4;</code>
     * @return int|string
     */
    public function getInferenceCount()
    {
        return $this->inference_count;
    }

    /**
     *&#64;&#64;  .. cpp:var:: uint64 last_inference
     *&#64;&#64;
     *&#64;&#64;     The cumulative count of successful inference requests made for this
     *&#64;&#64;     model. Each inference in a batched request is counted as an
     *&#64;&#64;     individual inference. For example, if a client sends a single
     *&#64;&#64;     inference request with batch size 64, "inference_count" will be
     *&#64;&#64;     incremented by 64. Similarly, if a clients sends 64 individual
     *&#64;&#64;     requests each with batch size 1, "inference_count" will be
     *&#64;&#64;     incremented by 64. The "inference_count" value DOES NOT include
     *&#64;&#64;     cache hits.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>uint64 inference_count = 4;</code>
     * @param int|string $var
     * @return $this
     */
    public function setInferenceCount($var)
    {
        GPBUtil::checkUint64($var);
        $this->inference_count = $var;

        return $this;
    }

    /**
     *&#64;&#64;  .. cpp:var:: uint64 last_inference
     *&#64;&#64;
     *&#64;&#64;     The cumulative count of the number of successful inference executions
     *&#64;&#64;     performed for the model. When dynamic batching is enabled, a single
     *&#64;&#64;     model execution can perform inferencing for more than one inference
     *&#64;&#64;     request. For example, if a clients sends 64 individual requests each
     *&#64;&#64;     with batch size 1 and the dynamic batcher batches them into a single
     *&#64;&#64;     large batch for model execution then "execution_count" will be
     *&#64;&#64;     incremented by 1. If, on the other hand, the dynamic batcher is not
     *&#64;&#64;     enabled for that each of the 64 individual requests is executed
     *&#64;&#64;     independently, then "execution_count" will be incremented by 64.
     *&#64;&#64;     The "execution_count" value DOES NOT include cache hits.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>uint64 execution_count = 5;</code>
     * @return int|string
     */
    public function getExecutionCount()
    {
        return $this->execution_count;
    }

    /**
     *&#64;&#64;  .. cpp:var:: uint64 last_inference
     *&#64;&#64;
     *&#64;&#64;     The cumulative count of the number of successful inference executions
     *&#64;&#64;     performed for the model. When dynamic batching is enabled, a single
     *&#64;&#64;     model execution can perform inferencing for more than one inference
     *&#64;&#64;     request. For example, if a clients sends 64 individual requests each
     *&#64;&#64;     with batch size 1 and the dynamic batcher batches them into a single
     *&#64;&#64;     large batch for model execution then "execution_count" will be
     *&#64;&#64;     incremented by 1. If, on the other hand, the dynamic batcher is not
     *&#64;&#64;     enabled for that each of the 64 individual requests is executed
     *&#64;&#64;     independently, then "execution_count" will be incremented by 64.
     *&#64;&#64;     The "execution_count" value DOES NOT include cache hits.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>uint64 execution_count = 5;</code>
     * @param int|string $var
     * @return $this
     */
    public function setExecutionCount($var)
    {
        GPBUtil::checkUint64($var);
        $this->execution_count = $var;

        return $this;
    }

    /**
     *&#64;&#64;  .. cpp:var:: InferStatistics inference_stats
     *&#64;&#64;
     *&#64;&#64;     The aggregate statistics for the model/version.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>.inference.InferStatistics inference_stats = 6;</code>
     * @return \Inference\InferStatistics
     */
    public function getInferenceStats()
    {
        return $this->inference_stats;
    }

    /**
     *&#64;&#64;  .. cpp:var:: InferStatistics inference_stats
     *&#64;&#64;
     *&#64;&#64;     The aggregate statistics for the model/version.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>.inference.InferStatistics inference_stats = 6;</code>
     * @param \Inference\InferStatistics $var
     * @return $this
     */
    public function setInferenceStats($var)
    {
        GPBUtil::checkMessage($var, \Inference\InferStatistics::class);
        $this->inference_stats = $var;

        return $this;
    }

    /**
     *&#64;&#64;  .. cpp:var:: InferBatchStatistics batch_stats (repeated)
     *&#64;&#64;
     *&#64;&#64;     The aggregate statistics for each different batch size that is
     *&#64;&#64;     executed in the model. The batch statistics indicate how many actual
     *&#64;&#64;     model executions were performed and show differences due to different
     *&#64;&#64;     batch size (for example, larger batches typically take longer to
     *&#64;&#64;     compute).
     *&#64;&#64;
     *
     * Generated from protobuf field <code>repeated .inference.InferBatchStatistics batch_stats = 7;</code>
     * @return \Google\Protobuf\Internal\RepeatedField
     */
    public function getBatchStats()
    {
        return $this->batch_stats;
    }

    /**
     *&#64;&#64;  .. cpp:var:: InferBatchStatistics batch_stats (repeated)
     *&#64;&#64;
     *&#64;&#64;     The aggregate statistics for each different batch size that is
     *&#64;&#64;     executed in the model. The batch statistics indicate how many actual
     *&#64;&#64;     model executions were performed and show differences due to different
     *&#64;&#64;     batch size (for example, larger batches typically take longer to
     *&#64;&#64;     compute).
     *&#64;&#64;
     *
     * Generated from protobuf field <code>repeated .inference.InferBatchStatistics batch_stats = 7;</code>
     * @param \Inference\InferBatchStatistics[]|\Google\Protobuf\Internal\RepeatedField $var
     * @return $this
     */
    public function setBatchStats($var)
    {
        $arr = GPBUtil::checkRepeatedField($var, \Google\Protobuf\Internal\GPBType::MESSAGE, \Inference\InferBatchStatistics::class);
        $this->batch_stats = $arr;

        return $this;
    }

    /**
     *&#64;&#64;  .. cpp:var:: MemoryUsage memory_usage (repeated)
     *&#64;&#64;
     *&#64;&#64;     The memory usage detected during model loading, which may be used to
     *&#64;&#64;     estimate the memory to be released once the model is unloaded. Note
     *&#64;&#64;     that the estimation is inferenced by the profiling tools and
     *&#64;&#64;     framework's memory schema, therefore it is advised to perform
     *&#64;&#64;     experiments to understand the scenario that the reported memory usage
     *&#64;&#64;     can be relied on. As a starting point, the GPU memory usage for
     *&#64;&#64;     models in ONNX Runtime backend and TensorRT backend is usually
     *&#64;&#64;     aligned.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>repeated .inference.MemoryUsage memory_usage = 8;</code>
     * @return \Google\Protobuf\Internal\RepeatedField
     */
    public function getMemoryUsage()
    {
        return $this->memory_usage;
    }

    /**
     *&#64;&#64;  .. cpp:var:: MemoryUsage memory_usage (repeated)
     *&#64;&#64;
     *&#64;&#64;     The memory usage detected during model loading, which may be used to
     *&#64;&#64;     estimate the memory to be released once the model is unloaded. Note
     *&#64;&#64;     that the estimation is inferenced by the profiling tools and
     *&#64;&#64;     framework's memory schema, therefore it is advised to perform
     *&#64;&#64;     experiments to understand the scenario that the reported memory usage
     *&#64;&#64;     can be relied on. As a starting point, the GPU memory usage for
     *&#64;&#64;     models in ONNX Runtime backend and TensorRT backend is usually
     *&#64;&#64;     aligned.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>repeated .inference.MemoryUsage memory_usage = 8;</code>
     * @param \Inference\MemoryUsage[]|\Google\Protobuf\Internal\RepeatedField $var
     * @return $this
     */
    public function setMemoryUsage($var)
    {
        $arr = GPBUtil::checkRepeatedField($var, \Google\Protobuf\Internal\GPBType::MESSAGE, \Inference\MemoryUsage::class);
        $this->memory_usage = $arr;

        return $this;
    }

    /**
     *&#64;&#64;  .. cpp:var:: map<string, InferResponseStatistics> response_stats
     *&#64;&#64;
     *&#64;&#64;     The key and value pairs for all responses statistics. The key is a
     *&#64;&#64;     string identifying a set of response statistics aggregated together
     *&#64;&#64;     (i.e. index of the response sent). The value is the aggregated
     *&#64;&#64;     response statistics.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>map<string, .inference.InferResponseStatistics> response_stats = 9;</code>
     * @return \Google\Protobuf\Internal\MapField
     */
    public function getResponseStats()
    {
        return $this->response_stats;
    }

    /**
     *&#64;&#64;  .. cpp:var:: map<string, InferResponseStatistics> response_stats
     *&#64;&#64;
     *&#64;&#64;     The key and value pairs for all responses statistics. The key is a
     *&#64;&#64;     string identifying a set of response statistics aggregated together
     *&#64;&#64;     (i.e. index of the response sent). The value is the aggregated
     *&#64;&#64;     response statistics.
     *&#64;&#64;
     *
     * Generated from protobuf field <code>map<string, .inference.InferResponseStatistics> response_stats = 9;</code>
     * @param array|\Google\Protobuf\Internal\MapField $var
     * @return $this
     */
    public function setResponseStats($var)
    {
        $arr = GPBUtil::checkMapField($var, \Google\Protobuf\Internal\GPBType::STRING, \Google\Protobuf\Internal\GPBType::MESSAGE, \Inference\InferResponseStatistics::class);
        $this->response_stats = $arr;

        return $this;
    }

}

