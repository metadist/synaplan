<?php
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# NO CHECKED-IN PROTOBUF GENCODE
# source: grpc_service.proto

namespace Inference;

use Google\Protobuf\Internal\GPBType;
use Google\Protobuf\Internal\GPBUtil;
use Google\Protobuf\RepeatedField;

/**
 *\@\@
 *\@\@.. cpp:var:: message ModelStatistics
 *\@\@
 *\@\@   Statistics for a specific model and version.
 *\@\@
 *
 * Generated from protobuf message <code>inference.ModelStatistics</code>
 */
class ModelStatistics extends \Google\Protobuf\Internal\Message
{
    /**
     *\@\@  .. cpp:var:: string name
     *\@\@
     *\@\@     The name of the model. If not given returns statistics for all
     *\@\@
     *
     * Generated from protobuf field <code>string name = 1;</code>
     */
    protected $name = '';
    /**
     *\@\@  .. cpp:var:: string version
     *\@\@
     *\@\@     The version of the model.
     *\@\@
     *
     * Generated from protobuf field <code>string version = 2;</code>
     */
    protected $version = '';
    /**
     *\@\@  .. cpp:var:: uint64 last_inference
     *\@\@
     *\@\@     The timestamp of the last inference request made for this model,
     *\@\@     as milliseconds since the epoch.
     *\@\@
     *
     * Generated from protobuf field <code>uint64 last_inference = 3;</code>
     */
    protected $last_inference = 0;
    /**
     *\@\@  .. cpp:var:: uint64 last_inference
     *\@\@
     *\@\@     The cumulative count of successful inference requests made for this
     *\@\@     model. Each inference in a batched request is counted as an
     *\@\@     individual inference. For example, if a client sends a single
     *\@\@     inference request with batch size 64, "inference_count" will be
     *\@\@     incremented by 64. Similarly, if a clients sends 64 individual
     *\@\@     requests each with batch size 1, "inference_count" will be
     *\@\@     incremented by 64. The "inference_count" value DOES NOT include
     *\@\@     cache hits.
     *\@\@
     *
     * Generated from protobuf field <code>uint64 inference_count = 4;</code>
     */
    protected $inference_count = 0;
    /**
     *\@\@  .. cpp:var:: uint64 last_inference
     *\@\@
     *\@\@     The cumulative count of the number of successful inference executions
     *\@\@     performed for the model. When dynamic batching is enabled, a single
     *\@\@     model execution can perform inferencing for more than one inference
     *\@\@     request. For example, if a clients sends 64 individual requests each
     *\@\@     with batch size 1 and the dynamic batcher batches them into a single
     *\@\@     large batch for model execution then "execution_count" will be
     *\@\@     incremented by 1. If, on the other hand, the dynamic batcher is not
     *\@\@     enabled for that each of the 64 individual requests is executed
     *\@\@     independently, then "execution_count" will be incremented by 64.
     *\@\@     The "execution_count" value DOES NOT include cache hits.
     *\@\@
     *
     * Generated from protobuf field <code>uint64 execution_count = 5;</code>
     */
    protected $execution_count = 0;
    /**
     *\@\@  .. cpp:var:: InferStatistics inference_stats
     *\@\@
     *\@\@     The aggregate statistics for the model/version.
     *\@\@
     *
     * Generated from protobuf field <code>.inference.InferStatistics inference_stats = 6;</code>
     */
    protected $inference_stats = null;
    /**
     *\@\@  .. cpp:var:: InferBatchStatistics batch_stats (repeated)
     *\@\@
     *\@\@     The aggregate statistics for each different batch size that is
     *\@\@     executed in the model. The batch statistics indicate how many actual
     *\@\@     model executions were performed and show differences due to different
     *\@\@     batch size (for example, larger batches typically take longer to
     *\@\@     compute).
     *\@\@
     *
     * Generated from protobuf field <code>repeated .inference.InferBatchStatistics batch_stats = 7;</code>
     */
    private $batch_stats;
    /**
     *\@\@  .. cpp:var:: MemoryUsage memory_usage (repeated)
     *\@\@
     *\@\@     The memory usage detected during model loading, which may be used to
     *\@\@     estimate the memory to be released once the model is unloaded. Note
     *\@\@     that the estimation is inferenced by the profiling tools and
     *\@\@     framework's memory schema, therefore it is advised to perform
     *\@\@     experiments to understand the scenario that the reported memory usage
     *\@\@     can be relied on. As a starting point, the GPU memory usage for
     *\@\@     models in ONNX Runtime backend and TensorRT backend is usually
     *\@\@     aligned.
     *\@\@
     *
     * Generated from protobuf field <code>repeated .inference.MemoryUsage memory_usage = 8;</code>
     */
    private $memory_usage;
    /**
     *\@\@  .. cpp:var:: map<string, InferResponseStatistics> response_stats
     *\@\@
     *\@\@     The key and value pairs for all responses statistics. The key is a
     *\@\@     string identifying a set of response statistics aggregated together
     *\@\@     (i.e. index of the response sent). The value is the aggregated
     *\@\@     response statistics.
     *\@\@
     *
     * Generated from protobuf field <code>map<string, .inference.InferResponseStatistics> response_stats = 9;</code>
     */
    private $response_stats;

    /**
     * Constructor.
     *
     * @param array $data {
     *     Optional. Data for populating the Message object.
     *
     *     @type string $name
     *          \@\@  .. cpp:var:: string name
     *          \@\@
     *          \@\@     The name of the model. If not given returns statistics for all
     *          \@\@
     *     @type string $version
     *          \@\@  .. cpp:var:: string version
     *          \@\@
     *          \@\@     The version of the model.
     *          \@\@
     *     @type int|string $last_inference
     *          \@\@  .. cpp:var:: uint64 last_inference
     *          \@\@
     *          \@\@     The timestamp of the last inference request made for this model,
     *          \@\@     as milliseconds since the epoch.
     *          \@\@
     *     @type int|string $inference_count
     *          \@\@  .. cpp:var:: uint64 last_inference
     *          \@\@
     *          \@\@     The cumulative count of successful inference requests made for this
     *          \@\@     model. Each inference in a batched request is counted as an
     *          \@\@     individual inference. For example, if a client sends a single
     *          \@\@     inference request with batch size 64, "inference_count" will be
     *          \@\@     incremented by 64. Similarly, if a clients sends 64 individual
     *          \@\@     requests each with batch size 1, "inference_count" will be
     *          \@\@     incremented by 64. The "inference_count" value DOES NOT include
     *          \@\@     cache hits.
     *          \@\@
     *     @type int|string $execution_count
     *          \@\@  .. cpp:var:: uint64 last_inference
     *          \@\@
     *          \@\@     The cumulative count of the number of successful inference executions
     *          \@\@     performed for the model. When dynamic batching is enabled, a single
     *          \@\@     model execution can perform inferencing for more than one inference
     *          \@\@     request. For example, if a clients sends 64 individual requests each
     *          \@\@     with batch size 1 and the dynamic batcher batches them into a single
     *          \@\@     large batch for model execution then "execution_count" will be
     *          \@\@     incremented by 1. If, on the other hand, the dynamic batcher is not
     *          \@\@     enabled for that each of the 64 individual requests is executed
     *          \@\@     independently, then "execution_count" will be incremented by 64.
     *          \@\@     The "execution_count" value DOES NOT include cache hits.
     *          \@\@
     *     @type \Inference\InferStatistics $inference_stats
     *          \@\@  .. cpp:var:: InferStatistics inference_stats
     *          \@\@
     *          \@\@     The aggregate statistics for the model/version.
     *          \@\@
     *     @type \Inference\InferBatchStatistics[] $batch_stats
     *          \@\@  .. cpp:var:: InferBatchStatistics batch_stats (repeated)
     *          \@\@
     *          \@\@     The aggregate statistics for each different batch size that is
     *          \@\@     executed in the model. The batch statistics indicate how many actual
     *          \@\@     model executions were performed and show differences due to different
     *          \@\@     batch size (for example, larger batches typically take longer to
     *          \@\@     compute).
     *          \@\@
     *     @type \Inference\MemoryUsage[] $memory_usage
     *          \@\@  .. cpp:var:: MemoryUsage memory_usage (repeated)
     *          \@\@
     *          \@\@     The memory usage detected during model loading, which may be used to
     *          \@\@     estimate the memory to be released once the model is unloaded. Note
     *          \@\@     that the estimation is inferenced by the profiling tools and
     *          \@\@     framework's memory schema, therefore it is advised to perform
     *          \@\@     experiments to understand the scenario that the reported memory usage
     *          \@\@     can be relied on. As a starting point, the GPU memory usage for
     *          \@\@     models in ONNX Runtime backend and TensorRT backend is usually
     *          \@\@     aligned.
     *          \@\@
     *     @type array|\Google\Protobuf\Internal\MapField $response_stats
     *          \@\@  .. cpp:var:: map<string, InferResponseStatistics> response_stats
     *          \@\@
     *          \@\@     The key and value pairs for all responses statistics. The key is a
     *          \@\@     string identifying a set of response statistics aggregated together
     *          \@\@     (i.e. index of the response sent). The value is the aggregated
     *          \@\@     response statistics.
     *          \@\@
     * }
     */
    public function __construct($data = NULL) {
        \GPBMetadata\GrpcService::initOnce();
        parent::__construct($data);
    }

    /**
     *\@\@  .. cpp:var:: string name
     *\@\@
     *\@\@     The name of the model. If not given returns statistics for all
     *\@\@
     *
     * Generated from protobuf field <code>string name = 1;</code>
     * @return string
     */
    public function getName()
    {
        return $this->name;
    }

    /**
     *\@\@  .. cpp:var:: string name
     *\@\@
     *\@\@     The name of the model. If not given returns statistics for all
     *\@\@
     *
     * Generated from protobuf field <code>string name = 1;</code>
     * @param string $var
     * @return $this
     */
    public function setName($var)
    {
        GPBUtil::checkString($var, True);
        $this->name = $var;

        return $this;
    }

    /**
     *\@\@  .. cpp:var:: string version
     *\@\@
     *\@\@     The version of the model.
     *\@\@
     *
     * Generated from protobuf field <code>string version = 2;</code>
     * @return string
     */
    public function getVersion()
    {
        return $this->version;
    }

    /**
     *\@\@  .. cpp:var:: string version
     *\@\@
     *\@\@     The version of the model.
     *\@\@
     *
     * Generated from protobuf field <code>string version = 2;</code>
     * @param string $var
     * @return $this
     */
    public function setVersion($var)
    {
        GPBUtil::checkString($var, True);
        $this->version = $var;

        return $this;
    }

    /**
     *\@\@  .. cpp:var:: uint64 last_inference
     *\@\@
     *\@\@     The timestamp of the last inference request made for this model,
     *\@\@     as milliseconds since the epoch.
     *\@\@
     *
     * Generated from protobuf field <code>uint64 last_inference = 3;</code>
     * @return int|string
     */
    public function getLastInference()
    {
        return $this->last_inference;
    }

    /**
     *\@\@  .. cpp:var:: uint64 last_inference
     *\@\@
     *\@\@     The timestamp of the last inference request made for this model,
     *\@\@     as milliseconds since the epoch.
     *\@\@
     *
     * Generated from protobuf field <code>uint64 last_inference = 3;</code>
     * @param int|string $var
     * @return $this
     */
    public function setLastInference($var)
    {
        GPBUtil::checkUint64($var);
        $this->last_inference = $var;

        return $this;
    }

    /**
     *\@\@  .. cpp:var:: uint64 last_inference
     *\@\@
     *\@\@     The cumulative count of successful inference requests made for this
     *\@\@     model. Each inference in a batched request is counted as an
     *\@\@     individual inference. For example, if a client sends a single
     *\@\@     inference request with batch size 64, "inference_count" will be
     *\@\@     incremented by 64. Similarly, if a clients sends 64 individual
     *\@\@     requests each with batch size 1, "inference_count" will be
     *\@\@     incremented by 64. The "inference_count" value DOES NOT include
     *\@\@     cache hits.
     *\@\@
     *
     * Generated from protobuf field <code>uint64 inference_count = 4;</code>
     * @return int|string
     */
    public function getInferenceCount()
    {
        return $this->inference_count;
    }

    /**
     *\@\@  .. cpp:var:: uint64 last_inference
     *\@\@
     *\@\@     The cumulative count of successful inference requests made for this
     *\@\@     model. Each inference in a batched request is counted as an
     *\@\@     individual inference. For example, if a client sends a single
     *\@\@     inference request with batch size 64, "inference_count" will be
     *\@\@     incremented by 64. Similarly, if a clients sends 64 individual
     *\@\@     requests each with batch size 1, "inference_count" will be
     *\@\@     incremented by 64. The "inference_count" value DOES NOT include
     *\@\@     cache hits.
     *\@\@
     *
     * Generated from protobuf field <code>uint64 inference_count = 4;</code>
     * @param int|string $var
     * @return $this
     */
    public function setInferenceCount($var)
    {
        GPBUtil::checkUint64($var);
        $this->inference_count = $var;

        return $this;
    }

    /**
     *\@\@  .. cpp:var:: uint64 last_inference
     *\@\@
     *\@\@     The cumulative count of the number of successful inference executions
     *\@\@     performed for the model. When dynamic batching is enabled, a single
     *\@\@     model execution can perform inferencing for more than one inference
     *\@\@     request. For example, if a clients sends 64 individual requests each
     *\@\@     with batch size 1 and the dynamic batcher batches them into a single
     *\@\@     large batch for model execution then "execution_count" will be
     *\@\@     incremented by 1. If, on the other hand, the dynamic batcher is not
     *\@\@     enabled for that each of the 64 individual requests is executed
     *\@\@     independently, then "execution_count" will be incremented by 64.
     *\@\@     The "execution_count" value DOES NOT include cache hits.
     *\@\@
     *
     * Generated from protobuf field <code>uint64 execution_count = 5;</code>
     * @return int|string
     */
    public function getExecutionCount()
    {
        return $this->execution_count;
    }

    /**
     *\@\@  .. cpp:var:: uint64 last_inference
     *\@\@
     *\@\@     The cumulative count of the number of successful inference executions
     *\@\@     performed for the model. When dynamic batching is enabled, a single
     *\@\@     model execution can perform inferencing for more than one inference
     *\@\@     request. For example, if a clients sends 64 individual requests each
     *\@\@     with batch size 1 and the dynamic batcher batches them into a single
     *\@\@     large batch for model execution then "execution_count" will be
     *\@\@     incremented by 1. If, on the other hand, the dynamic batcher is not
     *\@\@     enabled for that each of the 64 individual requests is executed
     *\@\@     independently, then "execution_count" will be incremented by 64.
     *\@\@     The "execution_count" value DOES NOT include cache hits.
     *\@\@
     *
     * Generated from protobuf field <code>uint64 execution_count = 5;</code>
     * @param int|string $var
     * @return $this
     */
    public function setExecutionCount($var)
    {
        GPBUtil::checkUint64($var);
        $this->execution_count = $var;

        return $this;
    }

    /**
     *\@\@  .. cpp:var:: InferStatistics inference_stats
     *\@\@
     *\@\@     The aggregate statistics for the model/version.
     *\@\@
     *
     * Generated from protobuf field <code>.inference.InferStatistics inference_stats = 6;</code>
     * @return \Inference\InferStatistics|null
     */
    public function getInferenceStats()
    {
        return $this->inference_stats;
    }

    public function hasInferenceStats()
    {
        return isset($this->inference_stats);
    }

    public function clearInferenceStats()
    {
        unset($this->inference_stats);
    }

    /**
     *\@\@  .. cpp:var:: InferStatistics inference_stats
     *\@\@
     *\@\@     The aggregate statistics for the model/version.
     *\@\@
     *
     * Generated from protobuf field <code>.inference.InferStatistics inference_stats = 6;</code>
     * @param \Inference\InferStatistics $var
     * @return $this
     */
    public function setInferenceStats($var)
    {
        GPBUtil::checkMessage($var, \Inference\InferStatistics::class);
        $this->inference_stats = $var;

        return $this;
    }

    /**
     *\@\@  .. cpp:var:: InferBatchStatistics batch_stats (repeated)
     *\@\@
     *\@\@     The aggregate statistics for each different batch size that is
     *\@\@     executed in the model. The batch statistics indicate how many actual
     *\@\@     model executions were performed and show differences due to different
     *\@\@     batch size (for example, larger batches typically take longer to
     *\@\@     compute).
     *\@\@
     *
     * Generated from protobuf field <code>repeated .inference.InferBatchStatistics batch_stats = 7;</code>
     * @return RepeatedField<\Inference\InferBatchStatistics>
     */
    public function getBatchStats()
    {
        return $this->batch_stats;
    }

    /**
     *\@\@  .. cpp:var:: InferBatchStatistics batch_stats (repeated)
     *\@\@
     *\@\@     The aggregate statistics for each different batch size that is
     *\@\@     executed in the model. The batch statistics indicate how many actual
     *\@\@     model executions were performed and show differences due to different
     *\@\@     batch size (for example, larger batches typically take longer to
     *\@\@     compute).
     *\@\@
     *
     * Generated from protobuf field <code>repeated .inference.InferBatchStatistics batch_stats = 7;</code>
     * @param \Inference\InferBatchStatistics[] $var
     * @return $this
     */
    public function setBatchStats($var)
    {
        $arr = GPBUtil::checkRepeatedField($var, \Google\Protobuf\Internal\GPBType::MESSAGE, \Inference\InferBatchStatistics::class);
        $this->batch_stats = $arr;

        return $this;
    }

    /**
     *\@\@  .. cpp:var:: MemoryUsage memory_usage (repeated)
     *\@\@
     *\@\@     The memory usage detected during model loading, which may be used to
     *\@\@     estimate the memory to be released once the model is unloaded. Note
     *\@\@     that the estimation is inferenced by the profiling tools and
     *\@\@     framework's memory schema, therefore it is advised to perform
     *\@\@     experiments to understand the scenario that the reported memory usage
     *\@\@     can be relied on. As a starting point, the GPU memory usage for
     *\@\@     models in ONNX Runtime backend and TensorRT backend is usually
     *\@\@     aligned.
     *\@\@
     *
     * Generated from protobuf field <code>repeated .inference.MemoryUsage memory_usage = 8;</code>
     * @return RepeatedField<\Inference\MemoryUsage>
     */
    public function getMemoryUsage()
    {
        return $this->memory_usage;
    }

    /**
     *\@\@  .. cpp:var:: MemoryUsage memory_usage (repeated)
     *\@\@
     *\@\@     The memory usage detected during model loading, which may be used to
     *\@\@     estimate the memory to be released once the model is unloaded. Note
     *\@\@     that the estimation is inferenced by the profiling tools and
     *\@\@     framework's memory schema, therefore it is advised to perform
     *\@\@     experiments to understand the scenario that the reported memory usage
     *\@\@     can be relied on. As a starting point, the GPU memory usage for
     *\@\@     models in ONNX Runtime backend and TensorRT backend is usually
     *\@\@     aligned.
     *\@\@
     *
     * Generated from protobuf field <code>repeated .inference.MemoryUsage memory_usage = 8;</code>
     * @param \Inference\MemoryUsage[] $var
     * @return $this
     */
    public function setMemoryUsage($var)
    {
        $arr = GPBUtil::checkRepeatedField($var, \Google\Protobuf\Internal\GPBType::MESSAGE, \Inference\MemoryUsage::class);
        $this->memory_usage = $arr;

        return $this;
    }

    /**
     *\@\@  .. cpp:var:: map<string, InferResponseStatistics> response_stats
     *\@\@
     *\@\@     The key and value pairs for all responses statistics. The key is a
     *\@\@     string identifying a set of response statistics aggregated together
     *\@\@     (i.e. index of the response sent). The value is the aggregated
     *\@\@     response statistics.
     *\@\@
     *
     * Generated from protobuf field <code>map<string, .inference.InferResponseStatistics> response_stats = 9;</code>
     * @return \Google\Protobuf\Internal\MapField
     */
    public function getResponseStats()
    {
        return $this->response_stats;
    }

    /**
     *\@\@  .. cpp:var:: map<string, InferResponseStatistics> response_stats
     *\@\@
     *\@\@     The key and value pairs for all responses statistics. The key is a
     *\@\@     string identifying a set of response statistics aggregated together
     *\@\@     (i.e. index of the response sent). The value is the aggregated
     *\@\@     response statistics.
     *\@\@
     *
     * Generated from protobuf field <code>map<string, .inference.InferResponseStatistics> response_stats = 9;</code>
     * @param array|\Google\Protobuf\Internal\MapField $var
     * @return $this
     */
    public function setResponseStats($var)
    {
        $arr = GPBUtil::checkMapField($var, \Google\Protobuf\Internal\GPBType::STRING, \Google\Protobuf\Internal\GPBType::MESSAGE, \Inference\InferResponseStatistics::class);
        $this->response_stats = $arr;

        return $this;
    }

}

