<?php
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# NO CHECKED-IN PROTOBUF GENCODE
# source: model_config.proto

namespace Inference;

use Google\Protobuf\Internal\GPBType;
use Google\Protobuf\Internal\GPBUtil;
use Google\Protobuf\RepeatedField;

/**
 *\@\@
 *\@\@.. cpp:var:: message ModelOptimizationPolicy
 *\@\@
 *\@\@   Optimization settings for a model. These settings control if/how a
 *\@\@   model is optimized and prioritized by the backend framework when
 *\@\@   it is loaded.
 *\@\@
 *
 * Generated from protobuf message <code>inference.ModelOptimizationPolicy</code>
 */
class ModelOptimizationPolicy extends \Google\Protobuf\Internal\Message
{
    /**
     *\@\@  .. cpp:var:: Graph graph
     *\@\@
     *\@\@     The graph optimization setting for the model. Optional.
     *\@\@
     *
     * Generated from protobuf field <code>.inference.ModelOptimizationPolicy.Graph graph = 1;</code>
     */
    protected $graph = null;
    /**
     *\@\@  .. cpp:var:: ModelPriority priority
     *\@\@
     *\@\@     The priority setting for the model. Optional.
     *\@\@
     *
     * Generated from protobuf field <code>.inference.ModelOptimizationPolicy.ModelPriority priority = 2;</code>
     */
    protected $priority = 0;
    /**
     *\@\@  .. cpp:var:: Cuda cuda
     *\@\@
     *\@\@     CUDA-specific optimization settings. Optional.
     *\@\@
     *
     * Generated from protobuf field <code>.inference.ModelOptimizationPolicy.Cuda cuda = 3;</code>
     */
    protected $cuda = null;
    /**
     *\@\@  .. cpp:var:: ExecutionAccelerators execution_accelerators
     *\@\@
     *\@\@     The accelerators used for the model. Optional.
     *\@\@
     *
     * Generated from protobuf field <code>.inference.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;</code>
     */
    protected $execution_accelerators = null;
    /**
     *\@\@  .. cpp:var:: PinnedMemoryBuffer input_pinned_memory
     *\@\@
     *\@\@     Use pinned memory buffer when the data transfer for inputs
     *\@\@     is between GPU memory and non-pinned system memory.
     *\@\@     Default is true.
     *\@\@
     *
     * Generated from protobuf field <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer input_pinned_memory = 5;</code>
     */
    protected $input_pinned_memory = null;
    /**
     *\@\@  .. cpp:var:: PinnedMemoryBuffer output_pinned_memory
     *\@\@
     *\@\@     Use pinned memory buffer when the data transfer for outputs
     *\@\@     is between GPU memory and non-pinned system memory.
     *\@\@     Default is true.
     *\@\@
     *
     * Generated from protobuf field <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer output_pinned_memory = 6;</code>
     */
    protected $output_pinned_memory = null;
    /**
     *\@\@  .. cpp:var:: uint32 gather_kernel_buffer_threshold
     *\@\@
     *\@\@     The backend may use a gather kernel to gather input data if the
     *\@\@     device has direct access to the source buffer and the destination
     *\@\@     buffer. In such case, the gather kernel will be used only if the
     *\@\@     number of buffers to be gathered is greater or equal to
     *\@\@     the specified value. If 0, the gather kernel will be disabled.
     *\@\@     Default value is 0.
     *\@\@     Currently only recognized by TensorRT backend.
     *\@\@
     *
     * Generated from protobuf field <code>uint32 gather_kernel_buffer_threshold = 7;</code>
     */
    protected $gather_kernel_buffer_threshold = 0;
    /**
     *\@\@  .. cpp:var:: bool eager_batching
     *\@\@
     *\@\@     Start preparing the next batch before the model instance is ready
     *\@\@     for the next inference. This option can be used to overlap the
     *\@\@     batch preparation with model execution, with the trade-off that
     *\@\@     the next batch might be smaller than what it could have been.
     *\@\@     Default value is false.
     *\@\@     Currently only recognized by TensorRT backend.
     *\@\@
     *
     * Generated from protobuf field <code>bool eager_batching = 8;</code>
     */
    protected $eager_batching = false;

    /**
     * Constructor.
     *
     * @param array $data {
     *     Optional. Data for populating the Message object.
     *
     *     @type \Inference\ModelOptimizationPolicy\Graph $graph
     *          \@\@  .. cpp:var:: Graph graph
     *          \@\@
     *          \@\@     The graph optimization setting for the model. Optional.
     *          \@\@
     *     @type int $priority
     *          \@\@  .. cpp:var:: ModelPriority priority
     *          \@\@
     *          \@\@     The priority setting for the model. Optional.
     *          \@\@
     *     @type \Inference\ModelOptimizationPolicy\Cuda $cuda
     *          \@\@  .. cpp:var:: Cuda cuda
     *          \@\@
     *          \@\@     CUDA-specific optimization settings. Optional.
     *          \@\@
     *     @type \Inference\ModelOptimizationPolicy\ExecutionAccelerators $execution_accelerators
     *          \@\@  .. cpp:var:: ExecutionAccelerators execution_accelerators
     *          \@\@
     *          \@\@     The accelerators used for the model. Optional.
     *          \@\@
     *     @type \Inference\ModelOptimizationPolicy\PinnedMemoryBuffer $input_pinned_memory
     *          \@\@  .. cpp:var:: PinnedMemoryBuffer input_pinned_memory
     *          \@\@
     *          \@\@     Use pinned memory buffer when the data transfer for inputs
     *          \@\@     is between GPU memory and non-pinned system memory.
     *          \@\@     Default is true.
     *          \@\@
     *     @type \Inference\ModelOptimizationPolicy\PinnedMemoryBuffer $output_pinned_memory
     *          \@\@  .. cpp:var:: PinnedMemoryBuffer output_pinned_memory
     *          \@\@
     *          \@\@     Use pinned memory buffer when the data transfer for outputs
     *          \@\@     is between GPU memory and non-pinned system memory.
     *          \@\@     Default is true.
     *          \@\@
     *     @type int $gather_kernel_buffer_threshold
     *          \@\@  .. cpp:var:: uint32 gather_kernel_buffer_threshold
     *          \@\@
     *          \@\@     The backend may use a gather kernel to gather input data if the
     *          \@\@     device has direct access to the source buffer and the destination
     *          \@\@     buffer. In such case, the gather kernel will be used only if the
     *          \@\@     number of buffers to be gathered is greater or equal to
     *          \@\@     the specified value. If 0, the gather kernel will be disabled.
     *          \@\@     Default value is 0.
     *          \@\@     Currently only recognized by TensorRT backend.
     *          \@\@
     *     @type bool $eager_batching
     *          \@\@  .. cpp:var:: bool eager_batching
     *          \@\@
     *          \@\@     Start preparing the next batch before the model instance is ready
     *          \@\@     for the next inference. This option can be used to overlap the
     *          \@\@     batch preparation with model execution, with the trade-off that
     *          \@\@     the next batch might be smaller than what it could have been.
     *          \@\@     Default value is false.
     *          \@\@     Currently only recognized by TensorRT backend.
     *          \@\@
     * }
     */
    public function __construct($data = NULL) {
        \GPBMetadata\ModelConfig::initOnce();
        parent::__construct($data);
    }

    /**
     *\@\@  .. cpp:var:: Graph graph
     *\@\@
     *\@\@     The graph optimization setting for the model. Optional.
     *\@\@
     *
     * Generated from protobuf field <code>.inference.ModelOptimizationPolicy.Graph graph = 1;</code>
     * @return \Inference\ModelOptimizationPolicy\Graph|null
     */
    public function getGraph()
    {
        return $this->graph;
    }

    public function hasGraph()
    {
        return isset($this->graph);
    }

    public function clearGraph()
    {
        unset($this->graph);
    }

    /**
     *\@\@  .. cpp:var:: Graph graph
     *\@\@
     *\@\@     The graph optimization setting for the model. Optional.
     *\@\@
     *
     * Generated from protobuf field <code>.inference.ModelOptimizationPolicy.Graph graph = 1;</code>
     * @param \Inference\ModelOptimizationPolicy\Graph $var
     * @return $this
     */
    public function setGraph($var)
    {
        GPBUtil::checkMessage($var, \Inference\ModelOptimizationPolicy\Graph::class);
        $this->graph = $var;

        return $this;
    }

    /**
     *\@\@  .. cpp:var:: ModelPriority priority
     *\@\@
     *\@\@     The priority setting for the model. Optional.
     *\@\@
     *
     * Generated from protobuf field <code>.inference.ModelOptimizationPolicy.ModelPriority priority = 2;</code>
     * @return int
     */
    public function getPriority()
    {
        return $this->priority;
    }

    /**
     *\@\@  .. cpp:var:: ModelPriority priority
     *\@\@
     *\@\@     The priority setting for the model. Optional.
     *\@\@
     *
     * Generated from protobuf field <code>.inference.ModelOptimizationPolicy.ModelPriority priority = 2;</code>
     * @param int $var
     * @return $this
     */
    public function setPriority($var)
    {
        GPBUtil::checkEnum($var, \Inference\ModelOptimizationPolicy\ModelPriority::class);
        $this->priority = $var;

        return $this;
    }

    /**
     *\@\@  .. cpp:var:: Cuda cuda
     *\@\@
     *\@\@     CUDA-specific optimization settings. Optional.
     *\@\@
     *
     * Generated from protobuf field <code>.inference.ModelOptimizationPolicy.Cuda cuda = 3;</code>
     * @return \Inference\ModelOptimizationPolicy\Cuda|null
     */
    public function getCuda()
    {
        return $this->cuda;
    }

    public function hasCuda()
    {
        return isset($this->cuda);
    }

    public function clearCuda()
    {
        unset($this->cuda);
    }

    /**
     *\@\@  .. cpp:var:: Cuda cuda
     *\@\@
     *\@\@     CUDA-specific optimization settings. Optional.
     *\@\@
     *
     * Generated from protobuf field <code>.inference.ModelOptimizationPolicy.Cuda cuda = 3;</code>
     * @param \Inference\ModelOptimizationPolicy\Cuda $var
     * @return $this
     */
    public function setCuda($var)
    {
        GPBUtil::checkMessage($var, \Inference\ModelOptimizationPolicy\Cuda::class);
        $this->cuda = $var;

        return $this;
    }

    /**
     *\@\@  .. cpp:var:: ExecutionAccelerators execution_accelerators
     *\@\@
     *\@\@     The accelerators used for the model. Optional.
     *\@\@
     *
     * Generated from protobuf field <code>.inference.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;</code>
     * @return \Inference\ModelOptimizationPolicy\ExecutionAccelerators|null
     */
    public function getExecutionAccelerators()
    {
        return $this->execution_accelerators;
    }

    public function hasExecutionAccelerators()
    {
        return isset($this->execution_accelerators);
    }

    public function clearExecutionAccelerators()
    {
        unset($this->execution_accelerators);
    }

    /**
     *\@\@  .. cpp:var:: ExecutionAccelerators execution_accelerators
     *\@\@
     *\@\@     The accelerators used for the model. Optional.
     *\@\@
     *
     * Generated from protobuf field <code>.inference.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;</code>
     * @param \Inference\ModelOptimizationPolicy\ExecutionAccelerators $var
     * @return $this
     */
    public function setExecutionAccelerators($var)
    {
        GPBUtil::checkMessage($var, \Inference\ModelOptimizationPolicy\ExecutionAccelerators::class);
        $this->execution_accelerators = $var;

        return $this;
    }

    /**
     *\@\@  .. cpp:var:: PinnedMemoryBuffer input_pinned_memory
     *\@\@
     *\@\@     Use pinned memory buffer when the data transfer for inputs
     *\@\@     is between GPU memory and non-pinned system memory.
     *\@\@     Default is true.
     *\@\@
     *
     * Generated from protobuf field <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer input_pinned_memory = 5;</code>
     * @return \Inference\ModelOptimizationPolicy\PinnedMemoryBuffer|null
     */
    public function getInputPinnedMemory()
    {
        return $this->input_pinned_memory;
    }

    public function hasInputPinnedMemory()
    {
        return isset($this->input_pinned_memory);
    }

    public function clearInputPinnedMemory()
    {
        unset($this->input_pinned_memory);
    }

    /**
     *\@\@  .. cpp:var:: PinnedMemoryBuffer input_pinned_memory
     *\@\@
     *\@\@     Use pinned memory buffer when the data transfer for inputs
     *\@\@     is between GPU memory and non-pinned system memory.
     *\@\@     Default is true.
     *\@\@
     *
     * Generated from protobuf field <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer input_pinned_memory = 5;</code>
     * @param \Inference\ModelOptimizationPolicy\PinnedMemoryBuffer $var
     * @return $this
     */
    public function setInputPinnedMemory($var)
    {
        GPBUtil::checkMessage($var, \Inference\ModelOptimizationPolicy\PinnedMemoryBuffer::class);
        $this->input_pinned_memory = $var;

        return $this;
    }

    /**
     *\@\@  .. cpp:var:: PinnedMemoryBuffer output_pinned_memory
     *\@\@
     *\@\@     Use pinned memory buffer when the data transfer for outputs
     *\@\@     is between GPU memory and non-pinned system memory.
     *\@\@     Default is true.
     *\@\@
     *
     * Generated from protobuf field <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer output_pinned_memory = 6;</code>
     * @return \Inference\ModelOptimizationPolicy\PinnedMemoryBuffer|null
     */
    public function getOutputPinnedMemory()
    {
        return $this->output_pinned_memory;
    }

    public function hasOutputPinnedMemory()
    {
        return isset($this->output_pinned_memory);
    }

    public function clearOutputPinnedMemory()
    {
        unset($this->output_pinned_memory);
    }

    /**
     *\@\@  .. cpp:var:: PinnedMemoryBuffer output_pinned_memory
     *\@\@
     *\@\@     Use pinned memory buffer when the data transfer for outputs
     *\@\@     is between GPU memory and non-pinned system memory.
     *\@\@     Default is true.
     *\@\@
     *
     * Generated from protobuf field <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer output_pinned_memory = 6;</code>
     * @param \Inference\ModelOptimizationPolicy\PinnedMemoryBuffer $var
     * @return $this
     */
    public function setOutputPinnedMemory($var)
    {
        GPBUtil::checkMessage($var, \Inference\ModelOptimizationPolicy\PinnedMemoryBuffer::class);
        $this->output_pinned_memory = $var;

        return $this;
    }

    /**
     *\@\@  .. cpp:var:: uint32 gather_kernel_buffer_threshold
     *\@\@
     *\@\@     The backend may use a gather kernel to gather input data if the
     *\@\@     device has direct access to the source buffer and the destination
     *\@\@     buffer. In such case, the gather kernel will be used only if the
     *\@\@     number of buffers to be gathered is greater or equal to
     *\@\@     the specified value. If 0, the gather kernel will be disabled.
     *\@\@     Default value is 0.
     *\@\@     Currently only recognized by TensorRT backend.
     *\@\@
     *
     * Generated from protobuf field <code>uint32 gather_kernel_buffer_threshold = 7;</code>
     * @return int
     */
    public function getGatherKernelBufferThreshold()
    {
        return $this->gather_kernel_buffer_threshold;
    }

    /**
     *\@\@  .. cpp:var:: uint32 gather_kernel_buffer_threshold
     *\@\@
     *\@\@     The backend may use a gather kernel to gather input data if the
     *\@\@     device has direct access to the source buffer and the destination
     *\@\@     buffer. In such case, the gather kernel will be used only if the
     *\@\@     number of buffers to be gathered is greater or equal to
     *\@\@     the specified value. If 0, the gather kernel will be disabled.
     *\@\@     Default value is 0.
     *\@\@     Currently only recognized by TensorRT backend.
     *\@\@
     *
     * Generated from protobuf field <code>uint32 gather_kernel_buffer_threshold = 7;</code>
     * @param int $var
     * @return $this
     */
    public function setGatherKernelBufferThreshold($var)
    {
        GPBUtil::checkUint32($var);
        $this->gather_kernel_buffer_threshold = $var;

        return $this;
    }

    /**
     *\@\@  .. cpp:var:: bool eager_batching
     *\@\@
     *\@\@     Start preparing the next batch before the model instance is ready
     *\@\@     for the next inference. This option can be used to overlap the
     *\@\@     batch preparation with model execution, with the trade-off that
     *\@\@     the next batch might be smaller than what it could have been.
     *\@\@     Default value is false.
     *\@\@     Currently only recognized by TensorRT backend.
     *\@\@
     *
     * Generated from protobuf field <code>bool eager_batching = 8;</code>
     * @return bool
     */
    public function getEagerBatching()
    {
        return $this->eager_batching;
    }

    /**
     *\@\@  .. cpp:var:: bool eager_batching
     *\@\@
     *\@\@     Start preparing the next batch before the model instance is ready
     *\@\@     for the next inference. This option can be used to overlap the
     *\@\@     batch preparation with model execution, with the trade-off that
     *\@\@     the next batch might be smaller than what it could have been.
     *\@\@     Default value is false.
     *\@\@     Currently only recognized by TensorRT backend.
     *\@\@
     *
     * Generated from protobuf field <code>bool eager_batching = 8;</code>
     * @param bool $var
     * @return $this
     */
    public function setEagerBatching($var)
    {
        GPBUtil::checkBool($var);
        $this->eager_batching = $var;

        return $this;
    }

}

