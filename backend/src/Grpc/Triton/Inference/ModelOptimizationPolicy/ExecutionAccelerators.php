<?php
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# NO CHECKED-IN PROTOBUF GENCODE
# source: model_config.proto

namespace Inference\ModelOptimizationPolicy;

use Google\Protobuf\Internal\GPBType;
use Google\Protobuf\Internal\GPBUtil;
use Google\Protobuf\RepeatedField;

/**
 *\@\@
 *\@\@  .. cpp:var:: message ExecutionAccelerators
 *\@\@
 *\@\@     Specify the preferred execution accelerators to be used to execute
 *\@\@     the model. Currently only recognized by ONNX Runtime backend and
 *\@\@     TensorFlow backend.
 *\@\@
 *\@\@     For ONNX Runtime backend, it will deploy the model with the execution
 *\@\@     accelerators by priority, the priority is determined based on the
 *\@\@     order that they are set, i.e. the provider at the front has highest
 *\@\@     priority. Overall, the priority will be in the following order:
 *\@\@         <gpu_execution_accelerator> (if instance is on GPU)
 *\@\@         CUDA Execution Provider     (if instance is on GPU)
 *\@\@         <cpu_execution_accelerator>
 *\@\@         Default CPU Execution Provider
 *\@\@
 *
 * Generated from protobuf message <code>inference.ModelOptimizationPolicy.ExecutionAccelerators</code>
 */
class ExecutionAccelerators extends \Google\Protobuf\Internal\Message
{
    /**
     *\@\@    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
     *\@\@
     *\@\@       The preferred execution provider to be used if the model instance
     *\@\@       is deployed on GPU.
     *\@\@
     *\@\@       For ONNX Runtime backend, possible value is "tensorrt" as name,
     *\@\@       and no parameters are required.
     *\@\@
     *\@\@       For TensorFlow backend, possible values are "tensorrt",
     *\@\@       "auto_mixed_precision", "gpu_io".
     *\@\@
     *\@\@       For "tensorrt", the following parameters can be specified:
     *\@\@         "precision_mode": The precision used for optimization.
     *\@\@         Allowed values are "FP32" and "FP16". Default value is "FP32".
     *\@\@
     *\@\@         "max_cached_engines": The maximum number of cached TensorRT
     *\@\@         engines in dynamic TensorRT ops. Default value is 100.
     *\@\@
     *\@\@         "minimum_segment_size": The smallest model subgraph that will
     *\@\@         be considered for optimization by TensorRT. Default value is 3.
     *\@\@
     *\@\@         "max_workspace_size_bytes": The maximum GPU memory the model
     *\@\@         can use temporarily during execution. Default value is 1GB.
     *\@\@
     *\@\@       For "auto_mixed_precision", no parameters are required. If set,
     *\@\@       the model will try to use FP16 for better performance.
     *\@\@       This optimization can not be set with "tensorrt".
     *\@\@
     *\@\@       For "gpu_io", no parameters are required. If set, the model will
     *\@\@       be executed using TensorFlow Callable API to set input and output
     *\@\@       tensors in GPU memory if possible, which can reduce data transfer
     *\@\@       overhead if the model is used in ensemble. However, the Callable
     *\@\@       object will be created on model creation and it will request all
     *\@\@       outputs for every model execution, which may impact the
     *\@\@       performance if a request does not require all outputs. This
     *\@\@       optimization will only take affect if the model instance is
     *\@\@       created with KIND_GPU.
     *\@\@
     *
     * Generated from protobuf field <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
     */
    private $gpu_execution_accelerator;
    /**
     *\@\@    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
     *\@\@
     *\@\@       The preferred execution provider to be used if the model instance
     *\@\@       is deployed on CPU.
     *\@\@
     *\@\@       For ONNX Runtime backend, possible value is "openvino" as name,
     *\@\@       and no parameters are required.
     *\@\@
     *
     * Generated from protobuf field <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
     */
    private $cpu_execution_accelerator;

    /**
     * Constructor.
     *
     * @param array $data {
     *     Optional. Data for populating the Message object.
     *
     *     @type \Inference\ModelOptimizationPolicy\ExecutionAccelerators\Accelerator[] $gpu_execution_accelerator
     *          \@\@    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
     *          \@\@
     *          \@\@       The preferred execution provider to be used if the model instance
     *          \@\@       is deployed on GPU.
     *          \@\@
     *          \@\@       For ONNX Runtime backend, possible value is "tensorrt" as name,
     *          \@\@       and no parameters are required.
     *          \@\@
     *          \@\@       For TensorFlow backend, possible values are "tensorrt",
     *          \@\@       "auto_mixed_precision", "gpu_io".
     *          \@\@
     *          \@\@       For "tensorrt", the following parameters can be specified:
     *          \@\@         "precision_mode": The precision used for optimization.
     *          \@\@         Allowed values are "FP32" and "FP16". Default value is "FP32".
     *          \@\@
     *          \@\@         "max_cached_engines": The maximum number of cached TensorRT
     *          \@\@         engines in dynamic TensorRT ops. Default value is 100.
     *          \@\@
     *          \@\@         "minimum_segment_size": The smallest model subgraph that will
     *          \@\@         be considered for optimization by TensorRT. Default value is 3.
     *          \@\@
     *          \@\@         "max_workspace_size_bytes": The maximum GPU memory the model
     *          \@\@         can use temporarily during execution. Default value is 1GB.
     *          \@\@
     *          \@\@       For "auto_mixed_precision", no parameters are required. If set,
     *          \@\@       the model will try to use FP16 for better performance.
     *          \@\@       This optimization can not be set with "tensorrt".
     *          \@\@
     *          \@\@       For "gpu_io", no parameters are required. If set, the model will
     *          \@\@       be executed using TensorFlow Callable API to set input and output
     *          \@\@       tensors in GPU memory if possible, which can reduce data transfer
     *          \@\@       overhead if the model is used in ensemble. However, the Callable
     *          \@\@       object will be created on model creation and it will request all
     *          \@\@       outputs for every model execution, which may impact the
     *          \@\@       performance if a request does not require all outputs. This
     *          \@\@       optimization will only take affect if the model instance is
     *          \@\@       created with KIND_GPU.
     *          \@\@
     *     @type \Inference\ModelOptimizationPolicy\ExecutionAccelerators\Accelerator[] $cpu_execution_accelerator
     *          \@\@    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
     *          \@\@
     *          \@\@       The preferred execution provider to be used if the model instance
     *          \@\@       is deployed on CPU.
     *          \@\@
     *          \@\@       For ONNX Runtime backend, possible value is "openvino" as name,
     *          \@\@       and no parameters are required.
     *          \@\@
     * }
     */
    public function __construct($data = NULL) {
        \GPBMetadata\ModelConfig::initOnce();
        parent::__construct($data);
    }

    /**
     *\@\@    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
     *\@\@
     *\@\@       The preferred execution provider to be used if the model instance
     *\@\@       is deployed on GPU.
     *\@\@
     *\@\@       For ONNX Runtime backend, possible value is "tensorrt" as name,
     *\@\@       and no parameters are required.
     *\@\@
     *\@\@       For TensorFlow backend, possible values are "tensorrt",
     *\@\@       "auto_mixed_precision", "gpu_io".
     *\@\@
     *\@\@       For "tensorrt", the following parameters can be specified:
     *\@\@         "precision_mode": The precision used for optimization.
     *\@\@         Allowed values are "FP32" and "FP16". Default value is "FP32".
     *\@\@
     *\@\@         "max_cached_engines": The maximum number of cached TensorRT
     *\@\@         engines in dynamic TensorRT ops. Default value is 100.
     *\@\@
     *\@\@         "minimum_segment_size": The smallest model subgraph that will
     *\@\@         be considered for optimization by TensorRT. Default value is 3.
     *\@\@
     *\@\@         "max_workspace_size_bytes": The maximum GPU memory the model
     *\@\@         can use temporarily during execution. Default value is 1GB.
     *\@\@
     *\@\@       For "auto_mixed_precision", no parameters are required. If set,
     *\@\@       the model will try to use FP16 for better performance.
     *\@\@       This optimization can not be set with "tensorrt".
     *\@\@
     *\@\@       For "gpu_io", no parameters are required. If set, the model will
     *\@\@       be executed using TensorFlow Callable API to set input and output
     *\@\@       tensors in GPU memory if possible, which can reduce data transfer
     *\@\@       overhead if the model is used in ensemble. However, the Callable
     *\@\@       object will be created on model creation and it will request all
     *\@\@       outputs for every model execution, which may impact the
     *\@\@       performance if a request does not require all outputs. This
     *\@\@       optimization will only take affect if the model instance is
     *\@\@       created with KIND_GPU.
     *\@\@
     *
     * Generated from protobuf field <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
     * @return RepeatedField<\Inference\ModelOptimizationPolicy\ExecutionAccelerators\Accelerator>
     */
    public function getGpuExecutionAccelerator()
    {
        return $this->gpu_execution_accelerator;
    }

    /**
     *\@\@    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
     *\@\@
     *\@\@       The preferred execution provider to be used if the model instance
     *\@\@       is deployed on GPU.
     *\@\@
     *\@\@       For ONNX Runtime backend, possible value is "tensorrt" as name,
     *\@\@       and no parameters are required.
     *\@\@
     *\@\@       For TensorFlow backend, possible values are "tensorrt",
     *\@\@       "auto_mixed_precision", "gpu_io".
     *\@\@
     *\@\@       For "tensorrt", the following parameters can be specified:
     *\@\@         "precision_mode": The precision used for optimization.
     *\@\@         Allowed values are "FP32" and "FP16". Default value is "FP32".
     *\@\@
     *\@\@         "max_cached_engines": The maximum number of cached TensorRT
     *\@\@         engines in dynamic TensorRT ops. Default value is 100.
     *\@\@
     *\@\@         "minimum_segment_size": The smallest model subgraph that will
     *\@\@         be considered for optimization by TensorRT. Default value is 3.
     *\@\@
     *\@\@         "max_workspace_size_bytes": The maximum GPU memory the model
     *\@\@         can use temporarily during execution. Default value is 1GB.
     *\@\@
     *\@\@       For "auto_mixed_precision", no parameters are required. If set,
     *\@\@       the model will try to use FP16 for better performance.
     *\@\@       This optimization can not be set with "tensorrt".
     *\@\@
     *\@\@       For "gpu_io", no parameters are required. If set, the model will
     *\@\@       be executed using TensorFlow Callable API to set input and output
     *\@\@       tensors in GPU memory if possible, which can reduce data transfer
     *\@\@       overhead if the model is used in ensemble. However, the Callable
     *\@\@       object will be created on model creation and it will request all
     *\@\@       outputs for every model execution, which may impact the
     *\@\@       performance if a request does not require all outputs. This
     *\@\@       optimization will only take affect if the model instance is
     *\@\@       created with KIND_GPU.
     *\@\@
     *
     * Generated from protobuf field <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
     * @param \Inference\ModelOptimizationPolicy\ExecutionAccelerators\Accelerator[] $var
     * @return $this
     */
    public function setGpuExecutionAccelerator($var)
    {
        $arr = GPBUtil::checkRepeatedField($var, \Google\Protobuf\Internal\GPBType::MESSAGE, \Inference\ModelOptimizationPolicy\ExecutionAccelerators\Accelerator::class);
        $this->gpu_execution_accelerator = $arr;

        return $this;
    }

    /**
     *\@\@    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
     *\@\@
     *\@\@       The preferred execution provider to be used if the model instance
     *\@\@       is deployed on CPU.
     *\@\@
     *\@\@       For ONNX Runtime backend, possible value is "openvino" as name,
     *\@\@       and no parameters are required.
     *\@\@
     *
     * Generated from protobuf field <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
     * @return RepeatedField<\Inference\ModelOptimizationPolicy\ExecutionAccelerators\Accelerator>
     */
    public function getCpuExecutionAccelerator()
    {
        return $this->cpu_execution_accelerator;
    }

    /**
     *\@\@    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
     *\@\@
     *\@\@       The preferred execution provider to be used if the model instance
     *\@\@       is deployed on CPU.
     *\@\@
     *\@\@       For ONNX Runtime backend, possible value is "openvino" as name,
     *\@\@       and no parameters are required.
     *\@\@
     *
     * Generated from protobuf field <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
     * @param \Inference\ModelOptimizationPolicy\ExecutionAccelerators\Accelerator[] $var
     * @return $this
     */
    public function setCpuExecutionAccelerator($var)
    {
        $arr = GPBUtil::checkRepeatedField($var, \Google\Protobuf\Internal\GPBType::MESSAGE, \Inference\ModelOptimizationPolicy\ExecutionAccelerators\Accelerator::class);
        $this->cpu_execution_accelerator = $arr;

        return $this;
    }

}

