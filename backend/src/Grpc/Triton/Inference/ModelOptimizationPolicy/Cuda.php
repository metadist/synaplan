<?php
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# NO CHECKED-IN PROTOBUF GENCODE
# source: model_config.proto

namespace Inference\ModelOptimizationPolicy;

use Google\Protobuf\Internal\GPBType;
use Google\Protobuf\Internal\GPBUtil;
use Google\Protobuf\RepeatedField;

/**
 *\@\@
 *\@\@  .. cpp:var:: message Cuda
 *\@\@
 *\@\@     CUDA-specific optimization settings.
 *\@\@
 *
 * Generated from protobuf message <code>inference.ModelOptimizationPolicy.Cuda</code>
 */
class Cuda extends \Google\Protobuf\Internal\Message
{
    /**
     *\@\@    .. cpp:var:: bool graphs
     *\@\@
     *\@\@       Use CUDA graphs API to capture model operations and execute
     *\@\@       them more efficiently. Default value is false.
     *\@\@       Currently only recognized by TensorRT backend.
     *\@\@
     *
     * Generated from protobuf field <code>bool graphs = 1;</code>
     */
    protected $graphs = false;
    /**
     *\@\@    .. cpp:var:: bool busy_wait_events
     *\@\@
     *\@\@       Use busy-waiting to synchronize CUDA events to achieve minimum
     *\@\@       latency from event complete to host thread to be notified, with
     *\@\@       the cost of high CPU load. Default value is false.
     *\@\@       Currently only recognized by TensorRT backend.
     *\@\@
     *
     * Generated from protobuf field <code>bool busy_wait_events = 2;</code>
     */
    protected $busy_wait_events = false;
    /**
     *\@\@    .. cpp:var:: GraphSpec graph_spec (repeated)
     *\@\@
     *\@\@       Specification of the CUDA graph to be captured. If not specified
     *\@\@       and 'graphs' is true, the default CUDA graphs will be captured
     *\@\@       based on model settings.
     *\@\@       Currently only recognized by TensorRT backend.
     *\@\@
     *
     * Generated from protobuf field <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
     */
    private $graph_spec;
    /**
     *\@\@    .. cpp:var:: bool output_copy_stream
     *\@\@
     *\@\@       Uses a CUDA stream separate from the inference stream to copy the
     *\@\@       output to host. However, be aware that setting this option to
     *\@\@       true will lead to an increase in the memory consumption of the
     *\@\@       model as Triton will allocate twice as much GPU memory for its
     *\@\@       I/O tensor buffers. Default value is false.
     *\@\@       Currently only recognized by TensorRT backend.
     *\@\@
     *
     * Generated from protobuf field <code>bool output_copy_stream = 4;</code>
     */
    protected $output_copy_stream = false;

    /**
     * Constructor.
     *
     * @param array $data {
     *     Optional. Data for populating the Message object.
     *
     *     @type bool $graphs
     *          \@\@    .. cpp:var:: bool graphs
     *          \@\@
     *          \@\@       Use CUDA graphs API to capture model operations and execute
     *          \@\@       them more efficiently. Default value is false.
     *          \@\@       Currently only recognized by TensorRT backend.
     *          \@\@
     *     @type bool $busy_wait_events
     *          \@\@    .. cpp:var:: bool busy_wait_events
     *          \@\@
     *          \@\@       Use busy-waiting to synchronize CUDA events to achieve minimum
     *          \@\@       latency from event complete to host thread to be notified, with
     *          \@\@       the cost of high CPU load. Default value is false.
     *          \@\@       Currently only recognized by TensorRT backend.
     *          \@\@
     *     @type \Inference\ModelOptimizationPolicy\Cuda\GraphSpec[] $graph_spec
     *          \@\@    .. cpp:var:: GraphSpec graph_spec (repeated)
     *          \@\@
     *          \@\@       Specification of the CUDA graph to be captured. If not specified
     *          \@\@       and 'graphs' is true, the default CUDA graphs will be captured
     *          \@\@       based on model settings.
     *          \@\@       Currently only recognized by TensorRT backend.
     *          \@\@
     *     @type bool $output_copy_stream
     *          \@\@    .. cpp:var:: bool output_copy_stream
     *          \@\@
     *          \@\@       Uses a CUDA stream separate from the inference stream to copy the
     *          \@\@       output to host. However, be aware that setting this option to
     *          \@\@       true will lead to an increase in the memory consumption of the
     *          \@\@       model as Triton will allocate twice as much GPU memory for its
     *          \@\@       I/O tensor buffers. Default value is false.
     *          \@\@       Currently only recognized by TensorRT backend.
     *          \@\@
     * }
     */
    public function __construct($data = NULL) {
        \GPBMetadata\ModelConfig::initOnce();
        parent::__construct($data);
    }

    /**
     *\@\@    .. cpp:var:: bool graphs
     *\@\@
     *\@\@       Use CUDA graphs API to capture model operations and execute
     *\@\@       them more efficiently. Default value is false.
     *\@\@       Currently only recognized by TensorRT backend.
     *\@\@
     *
     * Generated from protobuf field <code>bool graphs = 1;</code>
     * @return bool
     */
    public function getGraphs()
    {
        return $this->graphs;
    }

    /**
     *\@\@    .. cpp:var:: bool graphs
     *\@\@
     *\@\@       Use CUDA graphs API to capture model operations and execute
     *\@\@       them more efficiently. Default value is false.
     *\@\@       Currently only recognized by TensorRT backend.
     *\@\@
     *
     * Generated from protobuf field <code>bool graphs = 1;</code>
     * @param bool $var
     * @return $this
     */
    public function setGraphs($var)
    {
        GPBUtil::checkBool($var);
        $this->graphs = $var;

        return $this;
    }

    /**
     *\@\@    .. cpp:var:: bool busy_wait_events
     *\@\@
     *\@\@       Use busy-waiting to synchronize CUDA events to achieve minimum
     *\@\@       latency from event complete to host thread to be notified, with
     *\@\@       the cost of high CPU load. Default value is false.
     *\@\@       Currently only recognized by TensorRT backend.
     *\@\@
     *
     * Generated from protobuf field <code>bool busy_wait_events = 2;</code>
     * @return bool
     */
    public function getBusyWaitEvents()
    {
        return $this->busy_wait_events;
    }

    /**
     *\@\@    .. cpp:var:: bool busy_wait_events
     *\@\@
     *\@\@       Use busy-waiting to synchronize CUDA events to achieve minimum
     *\@\@       latency from event complete to host thread to be notified, with
     *\@\@       the cost of high CPU load. Default value is false.
     *\@\@       Currently only recognized by TensorRT backend.
     *\@\@
     *
     * Generated from protobuf field <code>bool busy_wait_events = 2;</code>
     * @param bool $var
     * @return $this
     */
    public function setBusyWaitEvents($var)
    {
        GPBUtil::checkBool($var);
        $this->busy_wait_events = $var;

        return $this;
    }

    /**
     *\@\@    .. cpp:var:: GraphSpec graph_spec (repeated)
     *\@\@
     *\@\@       Specification of the CUDA graph to be captured. If not specified
     *\@\@       and 'graphs' is true, the default CUDA graphs will be captured
     *\@\@       based on model settings.
     *\@\@       Currently only recognized by TensorRT backend.
     *\@\@
     *
     * Generated from protobuf field <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
     * @return RepeatedField<\Inference\ModelOptimizationPolicy\Cuda\GraphSpec>
     */
    public function getGraphSpec()
    {
        return $this->graph_spec;
    }

    /**
     *\@\@    .. cpp:var:: GraphSpec graph_spec (repeated)
     *\@\@
     *\@\@       Specification of the CUDA graph to be captured. If not specified
     *\@\@       and 'graphs' is true, the default CUDA graphs will be captured
     *\@\@       based on model settings.
     *\@\@       Currently only recognized by TensorRT backend.
     *\@\@
     *
     * Generated from protobuf field <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
     * @param \Inference\ModelOptimizationPolicy\Cuda\GraphSpec[] $var
     * @return $this
     */
    public function setGraphSpec($var)
    {
        $arr = GPBUtil::checkRepeatedField($var, \Google\Protobuf\Internal\GPBType::MESSAGE, \Inference\ModelOptimizationPolicy\Cuda\GraphSpec::class);
        $this->graph_spec = $arr;

        return $this;
    }

    /**
     *\@\@    .. cpp:var:: bool output_copy_stream
     *\@\@
     *\@\@       Uses a CUDA stream separate from the inference stream to copy the
     *\@\@       output to host. However, be aware that setting this option to
     *\@\@       true will lead to an increase in the memory consumption of the
     *\@\@       model as Triton will allocate twice as much GPU memory for its
     *\@\@       I/O tensor buffers. Default value is false.
     *\@\@       Currently only recognized by TensorRT backend.
     *\@\@
     *
     * Generated from protobuf field <code>bool output_copy_stream = 4;</code>
     * @return bool
     */
    public function getOutputCopyStream()
    {
        return $this->output_copy_stream;
    }

    /**
     *\@\@    .. cpp:var:: bool output_copy_stream
     *\@\@
     *\@\@       Uses a CUDA stream separate from the inference stream to copy the
     *\@\@       output to host. However, be aware that setting this option to
     *\@\@       true will lead to an increase in the memory consumption of the
     *\@\@       model as Triton will allocate twice as much GPU memory for its
     *\@\@       I/O tensor buffers. Default value is false.
     *\@\@       Currently only recognized by TensorRT backend.
     *\@\@
     *
     * Generated from protobuf field <code>bool output_copy_stream = 4;</code>
     * @param bool $var
     * @return $this
     */
    public function setOutputCopyStream($var)
    {
        GPBUtil::checkBool($var);
        $this->output_copy_stream = $var;

        return $this;
    }

}

