<?php
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# NO CHECKED-IN PROTOBUF GENCODE
# source: model_config.proto

namespace Inference\ModelSequenceBatching;

use Google\Protobuf\Internal\GPBType;
use Google\Protobuf\Internal\GPBUtil;
use Google\Protobuf\RepeatedField;

/**
 *\@\@  .. cpp:var:: message StrategyOldest
 *\@\@
 *\@\@     The sequence batcher maintains up to 'max_candidate_sequences'
 *\@\@     candidate sequences. 'max_candidate_sequences' can be greater
 *\@\@     than the model's 'max_batch_size'. For inferencing the batcher
 *\@\@     chooses from the candidate sequences up to 'max_batch_size'
 *\@\@     inference requests. Requests are chosen in an oldest-first
 *\@\@     manner across all candidate sequences. A given sequence is
 *\@\@     not guaranteed to be assigned to the same batch slot for
 *\@\@     all inference requests of that sequence.
 *\@\@
 *
 * Generated from protobuf message <code>inference.ModelSequenceBatching.StrategyOldest</code>
 */
class StrategyOldest extends \Google\Protobuf\Internal\Message
{
    /**
     *\@\@    .. cpp:var:: int32 max_candidate_sequences
     *\@\@
     *\@\@       Maximum number of candidate sequences that the batcher
     *\@\@       maintains. Excess sequences are kept in an ordered backlog
     *\@\@       and become candidates when existing candidate sequences
     *\@\@       complete.
     *\@\@
     *
     * Generated from protobuf field <code>int32 max_candidate_sequences = 1;</code>
     */
    protected $max_candidate_sequences = 0;
    /**
     *\@\@    .. cpp:var:: int32 preferred_batch_size (repeated)
     *\@\@
     *\@\@       Preferred batch sizes for dynamic batching of candidate
     *\@\@       sequences. If a batch of one of these sizes can be formed
     *\@\@       it will be executed immediately. If not specified a
     *\@\@       preferred batch size will be chosen automatically
     *\@\@       based on model and GPU characteristics.
     *\@\@
     *
     * Generated from protobuf field <code>repeated int32 preferred_batch_size = 2;</code>
     */
    private $preferred_batch_size;
    /**
     *\@\@    .. cpp:var:: uint64 max_queue_delay_microseconds
     *\@\@
     *\@\@       The maximum time, in microseconds, a candidate request
     *\@\@       will be delayed in the dynamic batch scheduling queue to
     *\@\@       wait for additional requests for batching. Default is 0.
     *\@\@
     *
     * Generated from protobuf field <code>uint64 max_queue_delay_microseconds = 3;</code>
     */
    protected $max_queue_delay_microseconds = 0;
    /**
     *\@\@    .. cpp:var:: bool preserve_ordering
     *\@\@
     *\@\@       Should the dynamic batcher preserve the ordering of responses to
     *\@\@       match the order of requests received by the scheduler. Default is
     *\@\@       false. If true, the responses will be returned in the same order
     *\@\@       as the order of requests sent to the scheduler. If false, the
     *\@\@       responses may be returned in arbitrary order. This option is
     *\@\@       specifically needed when a sequence of related inference requests
     *\@\@       (i.e. inference requests with the same correlation ID) are sent
     *\@\@       to the dynamic batcher to ensure that the sequence responses are
     *\@\@       in the correct order.
     *\@\@
     *\@\@       When using decoupled models, setting this to true may block the
     *\@\@       responses from independent sequences from being returned to the
     *\@\@       client until the previous request completes, hurting overall
     *\@\@       performance. If using GRPC streaming protocol, the stream
     *\@\@       ordering guarantee may be sufficient alone to ensure the
     *\@\@       responses for each sequence are returned in sequence-order
     *\@\@       without blocking based on independent requests, depending on the
     *\@\@       use case.
     *\@\@
     *
     * Generated from protobuf field <code>bool preserve_ordering = 4;</code>
     */
    protected $preserve_ordering = false;

    /**
     * Constructor.
     *
     * @param array $data {
     *     Optional. Data for populating the Message object.
     *
     *     @type int $max_candidate_sequences
     *          \@\@    .. cpp:var:: int32 max_candidate_sequences
     *          \@\@
     *          \@\@       Maximum number of candidate sequences that the batcher
     *          \@\@       maintains. Excess sequences are kept in an ordered backlog
     *          \@\@       and become candidates when existing candidate sequences
     *          \@\@       complete.
     *          \@\@
     *     @type int[] $preferred_batch_size
     *          \@\@    .. cpp:var:: int32 preferred_batch_size (repeated)
     *          \@\@
     *          \@\@       Preferred batch sizes for dynamic batching of candidate
     *          \@\@       sequences. If a batch of one of these sizes can be formed
     *          \@\@       it will be executed immediately. If not specified a
     *          \@\@       preferred batch size will be chosen automatically
     *          \@\@       based on model and GPU characteristics.
     *          \@\@
     *     @type int|string $max_queue_delay_microseconds
     *          \@\@    .. cpp:var:: uint64 max_queue_delay_microseconds
     *          \@\@
     *          \@\@       The maximum time, in microseconds, a candidate request
     *          \@\@       will be delayed in the dynamic batch scheduling queue to
     *          \@\@       wait for additional requests for batching. Default is 0.
     *          \@\@
     *     @type bool $preserve_ordering
     *          \@\@    .. cpp:var:: bool preserve_ordering
     *          \@\@
     *          \@\@       Should the dynamic batcher preserve the ordering of responses to
     *          \@\@       match the order of requests received by the scheduler. Default is
     *          \@\@       false. If true, the responses will be returned in the same order
     *          \@\@       as the order of requests sent to the scheduler. If false, the
     *          \@\@       responses may be returned in arbitrary order. This option is
     *          \@\@       specifically needed when a sequence of related inference requests
     *          \@\@       (i.e. inference requests with the same correlation ID) are sent
     *          \@\@       to the dynamic batcher to ensure that the sequence responses are
     *          \@\@       in the correct order.
     *          \@\@
     *          \@\@       When using decoupled models, setting this to true may block the
     *          \@\@       responses from independent sequences from being returned to the
     *          \@\@       client until the previous request completes, hurting overall
     *          \@\@       performance. If using GRPC streaming protocol, the stream
     *          \@\@       ordering guarantee may be sufficient alone to ensure the
     *          \@\@       responses for each sequence are returned in sequence-order
     *          \@\@       without blocking based on independent requests, depending on the
     *          \@\@       use case.
     *          \@\@
     * }
     */
    public function __construct($data = NULL) {
        \GPBMetadata\ModelConfig::initOnce();
        parent::__construct($data);
    }

    /**
     *\@\@    .. cpp:var:: int32 max_candidate_sequences
     *\@\@
     *\@\@       Maximum number of candidate sequences that the batcher
     *\@\@       maintains. Excess sequences are kept in an ordered backlog
     *\@\@       and become candidates when existing candidate sequences
     *\@\@       complete.
     *\@\@
     *
     * Generated from protobuf field <code>int32 max_candidate_sequences = 1;</code>
     * @return int
     */
    public function getMaxCandidateSequences()
    {
        return $this->max_candidate_sequences;
    }

    /**
     *\@\@    .. cpp:var:: int32 max_candidate_sequences
     *\@\@
     *\@\@       Maximum number of candidate sequences that the batcher
     *\@\@       maintains. Excess sequences are kept in an ordered backlog
     *\@\@       and become candidates when existing candidate sequences
     *\@\@       complete.
     *\@\@
     *
     * Generated from protobuf field <code>int32 max_candidate_sequences = 1;</code>
     * @param int $var
     * @return $this
     */
    public function setMaxCandidateSequences($var)
    {
        GPBUtil::checkInt32($var);
        $this->max_candidate_sequences = $var;

        return $this;
    }

    /**
     *\@\@    .. cpp:var:: int32 preferred_batch_size (repeated)
     *\@\@
     *\@\@       Preferred batch sizes for dynamic batching of candidate
     *\@\@       sequences. If a batch of one of these sizes can be formed
     *\@\@       it will be executed immediately. If not specified a
     *\@\@       preferred batch size will be chosen automatically
     *\@\@       based on model and GPU characteristics.
     *\@\@
     *
     * Generated from protobuf field <code>repeated int32 preferred_batch_size = 2;</code>
     * @return RepeatedField<int>
     */
    public function getPreferredBatchSize()
    {
        return $this->preferred_batch_size;
    }

    /**
     *\@\@    .. cpp:var:: int32 preferred_batch_size (repeated)
     *\@\@
     *\@\@       Preferred batch sizes for dynamic batching of candidate
     *\@\@       sequences. If a batch of one of these sizes can be formed
     *\@\@       it will be executed immediately. If not specified a
     *\@\@       preferred batch size will be chosen automatically
     *\@\@       based on model and GPU characteristics.
     *\@\@
     *
     * Generated from protobuf field <code>repeated int32 preferred_batch_size = 2;</code>
     * @param int[] $var
     * @return $this
     */
    public function setPreferredBatchSize($var)
    {
        $arr = GPBUtil::checkRepeatedField($var, \Google\Protobuf\Internal\GPBType::INT32);
        $this->preferred_batch_size = $arr;

        return $this;
    }

    /**
     *\@\@    .. cpp:var:: uint64 max_queue_delay_microseconds
     *\@\@
     *\@\@       The maximum time, in microseconds, a candidate request
     *\@\@       will be delayed in the dynamic batch scheduling queue to
     *\@\@       wait for additional requests for batching. Default is 0.
     *\@\@
     *
     * Generated from protobuf field <code>uint64 max_queue_delay_microseconds = 3;</code>
     * @return int|string
     */
    public function getMaxQueueDelayMicroseconds()
    {
        return $this->max_queue_delay_microseconds;
    }

    /**
     *\@\@    .. cpp:var:: uint64 max_queue_delay_microseconds
     *\@\@
     *\@\@       The maximum time, in microseconds, a candidate request
     *\@\@       will be delayed in the dynamic batch scheduling queue to
     *\@\@       wait for additional requests for batching. Default is 0.
     *\@\@
     *
     * Generated from protobuf field <code>uint64 max_queue_delay_microseconds = 3;</code>
     * @param int|string $var
     * @return $this
     */
    public function setMaxQueueDelayMicroseconds($var)
    {
        GPBUtil::checkUint64($var);
        $this->max_queue_delay_microseconds = $var;

        return $this;
    }

    /**
     *\@\@    .. cpp:var:: bool preserve_ordering
     *\@\@
     *\@\@       Should the dynamic batcher preserve the ordering of responses to
     *\@\@       match the order of requests received by the scheduler. Default is
     *\@\@       false. If true, the responses will be returned in the same order
     *\@\@       as the order of requests sent to the scheduler. If false, the
     *\@\@       responses may be returned in arbitrary order. This option is
     *\@\@       specifically needed when a sequence of related inference requests
     *\@\@       (i.e. inference requests with the same correlation ID) are sent
     *\@\@       to the dynamic batcher to ensure that the sequence responses are
     *\@\@       in the correct order.
     *\@\@
     *\@\@       When using decoupled models, setting this to true may block the
     *\@\@       responses from independent sequences from being returned to the
     *\@\@       client until the previous request completes, hurting overall
     *\@\@       performance. If using GRPC streaming protocol, the stream
     *\@\@       ordering guarantee may be sufficient alone to ensure the
     *\@\@       responses for each sequence are returned in sequence-order
     *\@\@       without blocking based on independent requests, depending on the
     *\@\@       use case.
     *\@\@
     *
     * Generated from protobuf field <code>bool preserve_ordering = 4;</code>
     * @return bool
     */
    public function getPreserveOrdering()
    {
        return $this->preserve_ordering;
    }

    /**
     *\@\@    .. cpp:var:: bool preserve_ordering
     *\@\@
     *\@\@       Should the dynamic batcher preserve the ordering of responses to
     *\@\@       match the order of requests received by the scheduler. Default is
     *\@\@       false. If true, the responses will be returned in the same order
     *\@\@       as the order of requests sent to the scheduler. If false, the
     *\@\@       responses may be returned in arbitrary order. This option is
     *\@\@       specifically needed when a sequence of related inference requests
     *\@\@       (i.e. inference requests with the same correlation ID) are sent
     *\@\@       to the dynamic batcher to ensure that the sequence responses are
     *\@\@       in the correct order.
     *\@\@
     *\@\@       When using decoupled models, setting this to true may block the
     *\@\@       responses from independent sequences from being returned to the
     *\@\@       client until the previous request completes, hurting overall
     *\@\@       performance. If using GRPC streaming protocol, the stream
     *\@\@       ordering guarantee may be sufficient alone to ensure the
     *\@\@       responses for each sequence are returned in sequence-order
     *\@\@       without blocking based on independent requests, depending on the
     *\@\@       use case.
     *\@\@
     *
     * Generated from protobuf field <code>bool preserve_ordering = 4;</code>
     * @param bool $var
     * @return $this
     */
    public function setPreserveOrdering($var)
    {
        GPBUtil::checkBool($var);
        $this->preserve_ordering = $var;

        return $this;
    }

}

